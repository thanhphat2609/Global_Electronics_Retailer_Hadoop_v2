{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ca5f415",
   "metadata": {
    "papermill": {
     "duration": 0.002264,
     "end_time": "2024-06-29T05:01:00.804759",
     "exception": false,
     "start_time": "2024-06-29T05:01:00.802495",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Config connection to Apache Spark local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7d48d20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T05:01:00.813141Z",
     "iopub.status.busy": "2024-06-29T05:01:00.812787Z",
     "iopub.status.idle": "2024-06-29T05:01:00.871691Z",
     "shell.execute_reply": "2024-06-29T05:01:00.872007Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.06574,
     "end_time": "2024-06-29T05:01:00.872116",
     "exception": false,
     "start_time": "2024-06-29T05:01:00.806376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(spark_home = \"/home/thanhphat/BigData/spark-3.5.0-bin-hadoop3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebbb4d19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T05:01:00.877901Z",
     "iopub.status.busy": "2024-06-29T05:01:00.877431Z",
     "iopub.status.idle": "2024-06-29T05:01:01.332649Z",
     "shell.execute_reply": "2024-06-29T05:01:01.333130Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.459451,
     "end_time": "2024-06-29T05:01:01.333271",
     "exception": false,
     "start_time": "2024-06-29T05:01:00.873820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import traceback\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77131789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T05:01:01.340982Z",
     "iopub.status.busy": "2024-06-29T05:01:01.340582Z",
     "iopub.status.idle": "2024-06-29T05:01:01.342082Z",
     "shell.execute_reply": "2024-06-29T05:01:01.342420Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.007306,
     "end_time": "2024-06-29T05:01:01.342523",
     "exception": false,
     "start_time": "2024-06-29T05:01:01.335217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_name = \"Global_Electronics_Retailer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40cd9518",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T05:01:01.352353Z",
     "iopub.status.busy": "2024-06-29T05:01:01.351631Z",
     "iopub.status.idle": "2024-06-29T05:01:16.564983Z",
     "shell.execute_reply": "2024-06-29T05:01:16.565287Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 15.220987,
     "end_time": "2024-06-29T05:01:16.565390",
     "exception": false,
     "start_time": "2024-06-29T05:01:01.344403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:10 WARN Utils: Your hostname, thanhphat-inspiron-5406-2n1 resolves to a loopback address: 127.0.1.1; using 192.168.1.8 instead (on interface wlp0s20f3)\n",
      "24/06/29 12:01:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:12 WARN DependencyUtils: Local jar /driver/mysql-connector-j-8.1.0.jar does not exist, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:12 INFO SparkContext: Running Spark version 3.5.0\n",
      "24/06/29 12:01:12 INFO SparkContext: OS info Linux, 6.5.0-35-generic, amd64\n",
      "24/06/29 12:01:12 INFO SparkContext: Java version 1.8.0_412\n",
      "24/06/29 12:01:12 INFO ResourceUtils: ==============================================================\n",
      "24/06/29 12:01:12 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/06/29 12:01:12 INFO ResourceUtils: ==============================================================\n",
      "24/06/29 12:01:12 INFO SparkContext: Submitted application: Source_to_Bronze\n",
      "24/06/29 12:01:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/06/29 12:01:12 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor\n",
      "24/06/29 12:01:12 INFO ResourceProfileManager: Added ResourceProfile id: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:13 INFO SecurityManager: Changing view acls to: thanhphat\n",
      "24/06/29 12:01:13 INFO SecurityManager: Changing modify acls to: thanhphat\n",
      "24/06/29 12:01:13 INFO SecurityManager: Changing view acls groups to: \n",
      "24/06/29 12:01:13 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/06/29 12:01:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: thanhphat; groups with view permissions: EMPTY; users with modify permissions: thanhphat; groups with modify permissions: EMPTY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:13 INFO Utils: Successfully started service 'sparkDriver' on port 37087.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:14 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/06/29 12:01:14 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/06/29 12:01:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/06/29 12:01:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/06/29 12:01:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dd268f97-2060-4cb5-a0dc-b94bd2e3e611\n",
      "24/06/29 12:01:14 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB\n",
      "24/06/29 12:01:14 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:14 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/06/29 12:01:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/06/29 12:01:15 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/06/29 12:01:15 INFO Utils: Successfully started service 'SparkUI' on port 4042.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:15 ERROR SparkContext: Failed to add ../driver/mysql-connector-j-8.1.0.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /driver/mysql-connector-j-8.1.0.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2100)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2156)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:526)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:526)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:526)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/06/29 12:01:15 INFO Executor: Starting executor ID driver on host 192.168.1.8\n",
      "24/06/29 12:01:15 INFO Executor: OS info Linux, 6.5.0-35-generic, amd64\n",
      "24/06/29 12:01:15 INFO Executor: Java version 1.8.0_412\n",
      "24/06/29 12:01:15 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "24/06/29 12:01:15 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@240eb60f for default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45173.\n",
      "24/06/29 12:01:15 INFO NettyBlockTransferService: Server created on 192.168.1.8:45173\n",
      "24/06/29 12:01:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/06/29 12:01:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.8, 45173, None)\n",
      "24/06/29 12:01:15 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.8:45173 with 912.3 MiB RAM, BlockManagerId(driver, 192.168.1.8, 45173, None)\n",
      "24/06/29 12:01:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.8, 45173, None)\n",
      "24/06/29 12:01:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.8, 45173, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.executor.memory', '2g'),\n",
       " ('spark.master', 'local[4]'),\n",
       " ('spark.app.submitTime', '1719637272163'),\n",
       " ('spark.driver.host', '192.168.1.8'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.startTime', '1719637272708'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.jars', '../driver/mysql-connector-j-8.1.0.jar'),\n",
       " ('spark.sql.shuffle.partitions', '100'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.app.id', 'local-1719637275257'),\n",
       " ('spark.app.name', 'Source_to_Bronze'),\n",
       " ('spark.sql.parquet.vorder.enabled', 'true'),\n",
       " ('spark.repl.local.jars', 'file:/driver/mysql-connector-j-8.1.0.jar'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.instances', '2'),\n",
       " ('spark.driver.memory', '2g'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.driver.port', '37087')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config\n",
    "    # Number of executor: 2\n",
    "    # 2 CPU for each executor\n",
    "    # 2g memory for each executor\n",
    "\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.master(\"local[4]\") \\\n",
    "                            .appName(\"Source_to_Bronze\") \\\n",
    "                            .config(\"spark.sql.parquet.vorder.enabled\", \"true\") \\\n",
    "                            .config(\"spark.sql.shuffle.partitions\", 100) \\\n",
    "                            .config(\"spark.driver.memory\", \"2g\") \\\n",
    "                            .config(\"spark.executor.instances\", \"2\") \\\n",
    "                            .config(\"spark.executor.cores\", \"2\") \\\n",
    "                            .config(\"spark.executor.memory\", \"2g\") \\\n",
    "                            .config(\"spark.jars\", \"../driver/mysql-connector-j-8.1.0.jar\") \\\n",
    "                            .getOrCreate()\n",
    "\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e2f826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T05:01:16.578519Z",
     "iopub.status.busy": "2024-06-29T05:01:16.577989Z",
     "iopub.status.idle": "2024-06-29T05:01:20.090705Z",
     "shell.execute_reply": "2024-06-29T05:01:20.090223Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.521698,
     "end_time": "2024-06-29T05:01:20.090831",
     "exception": false,
     "start_time": "2024-06-29T05:01:16.569133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/thanhphat/PersonalProject/Global_Electronics_Retailer/source\")\n",
    "\n",
    "from modules.Extraction import *\n",
    "from modules.HDFSUtils import *\n",
    "from modules.LogUtils import *\n",
    "from modules.Metadata import *\n",
    "\n",
    "\n",
    "# Instance for modules\n",
    "extraction = Extraction()\n",
    "hdfsUtils = HDFSUtils()\n",
    "logUtils = LogUtils() \n",
    "metadata = Metadata()\n",
    "\n",
    "# Define base_path\n",
    "file_path = f\"hdfs://localhost:9000/lakehouse/LH_{project_name}/Files/Bronze\"\n",
    "log_path = f\"hdfs://localhost:9000/lakehouse/LH_{project_name}/Files/log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c136d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T05:01:20.100386Z",
     "iopub.status.busy": "2024-06-29T05:01:20.099967Z",
     "iopub.status.idle": "2024-06-29T05:01:31.740068Z",
     "shell.execute_reply": "2024-06-29T05:01:31.740561Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 11.646962,
     "end_time": "2024-06-29T05:01:31.740813",
     "exception": false,
     "start_time": "2024-06-29T05:01:20.093851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/06/29 12:01:20 INFO SharedState: Warehouse path is 'file:/spark-warehouse'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:28 INFO CodeGenerator: Code generated in 772.302758 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:29 INFO SparkContext: Starting job: collect at /tmp/ipykernel_62125/1652265789.py:1\n",
      "24/06/29 12:01:29 INFO DAGScheduler: Got job 0 (collect at /tmp/ipykernel_62125/1652265789.py:1) with 1 output partitions\n",
      "24/06/29 12:01:29 INFO DAGScheduler: Final stage: ResultStage 0 (collect at /tmp/ipykernel_62125/1652265789.py:1)\n",
      "24/06/29 12:01:29 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:01:29 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:01:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collect at /tmp/ipykernel_62125/1652265789.py:1), which has no missing parents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 7.4 KiB, free 912.3 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 912.3 MiB)\n",
      "24/06/29 12:01:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.8:45173 (size: 3.8 KiB, free: 912.3 MiB)\n",
      "24/06/29 12:01:30 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580\n",
      "\r",
      "[Stage 0:>                                                          (0 + 0) / 1]\r",
      "24/06/29 12:01:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collect at /tmp/ipykernel_62125/1652265789.py:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:01:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7823 bytes) \n",
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "24/06/29 12:01:30 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:31 INFO CodeGenerator: Code generated in 10.946514 ms\n",
      "24/06/29 12:01:31 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1548 bytes result sent to driver\n",
      "24/06/29 12:01:31 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1001 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:01:31 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:01:31 INFO DAGScheduler: ResultStage 0 (collect at /tmp/ipykernel_62125/1652265789.py:1) finished in 1.809 s\n",
      "24/06/29 12:01:31 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:01:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/06/29 12:01:31 INFO DAGScheduler: Job 0 finished: collect at /tmp/ipykernel_62125/1652265789.py:1, took 1.967394 s\n",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "executionDate = str(spark.sql(\"SELECT CURRENT_DATE()\").collect()[0][0])\n",
    "\n",
    "# Partition Execution Date\n",
    "parse_execution = executionDate.split(\"-\")\n",
    "year = parse_execution[0]\n",
    "month = parse_execution[1]\n",
    "day = parse_execution[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf02f6e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T05:01:31.785802Z",
     "iopub.status.busy": "2024-06-29T05:01:31.785377Z",
     "iopub.status.idle": "2024-06-29T05:01:31.787296Z",
     "shell.execute_reply": "2024-06-29T05:01:31.787717Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.017161,
     "end_time": "2024-06-29T05:01:31.788064",
     "exception": false,
     "start_time": "2024-06-29T05:01:31.770903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Define parameter for connect to MySQL\n",
    "# database = \"Global_Electronics_Retailer\"\n",
    "# dbname = f\"jdbc:mysql://localhost:3306/{database}\"\n",
    "# driver = \"com.mysql.jdbc.Driver\"\n",
    "# username = \"root\"\n",
    "# password = \"password\"\n",
    "\n",
    "# df = extraction.read_table_mysql(spark, driver, dbname, \"customers\", username, password)\n",
    "\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2705c9",
   "metadata": {
    "papermill": {
     "duration": 0.01469,
     "end_time": "2024-06-29T05:01:31.815141",
     "exception": false,
     "start_time": "2024-06-29T05:01:31.800451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Metadata Table Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22193225",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T05:01:31.848680Z",
     "iopub.status.busy": "2024-06-29T05:01:31.847560Z",
     "iopub.status.idle": "2024-06-29T05:06:33.039059Z",
     "shell.execute_reply": "2024-06-29T05:06:33.039953Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 301.213065,
     "end_time": "2024-06-29T05:06:33.040175",
     "exception": false,
     "start_time": "2024-06-29T05:01:31.827110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:54 INFO CodeGenerator: Code generated in 9.732107 ms\n",
      "24/06/29 12:01:54 INFO SparkContext: Starting job: collect at /tmp/ipykernel_62125/3616262314.py:43\n",
      "24/06/29 12:01:54 INFO DAGScheduler: Got job 1 (collect at /tmp/ipykernel_62125/3616262314.py:43) with 1 output partitions\n",
      "24/06/29 12:01:54 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /tmp/ipykernel_62125/3616262314.py:43)\n",
      "24/06/29 12:01:54 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:01:54 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:01:54 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at collect at /tmp/ipykernel_62125/3616262314.py:43), which has no missing parents\n",
      "24/06/29 12:01:54 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.4 KiB, free 912.3 MiB)\n",
      "24/06/29 12:01:54 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 912.3 MiB)\n",
      "24/06/29 12:01:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.8:45173 (size: 3.8 KiB, free: 912.3 MiB)\n",
      "24/06/29 12:01:54 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:01:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at collect at /tmp/ipykernel_62125/3616262314.py:43) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:01:54 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:01:54 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7823 bytes) \n",
      "24/06/29 12:01:54 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "24/06/29 12:01:54 INFO CodeGenerator: Code generated in 15.653115 ms\n",
      "24/06/29 12:01:54 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1462 bytes result sent to driver\n",
      "24/06/29 12:01:54 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 28 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:01:54 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:01:54 INFO DAGScheduler: ResultStage 1 (collect at /tmp/ipykernel_62125/3616262314.py:43) finished in 0.039 s\n",
      "24/06/29 12:01:54 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:01:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/06/29 12:01:54 INFO DAGScheduler: Job 1 finished: collect at /tmp/ipykernel_62125/3616262314.py:43, took 0.044045 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:55 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.8:45173 in memory (size: 3.8 KiB, free: 912.3 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:59 INFO CodeGenerator: Code generated in 13.650753 ms\n",
      "24/06/29 12:01:59 INFO DAGScheduler: Registering RDD 7 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "24/06/29 12:01:59 INFO DAGScheduler: Got map stage job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:01:59 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:01:59 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:01:59 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:01:59 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[7] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:01:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.8 KiB, free 912.3 MiB)\n",
      "24/06/29 12:01:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 912.3 MiB)\n",
      "24/06/29 12:01:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.8:45173 (size: 7.2 KiB, free: 912.3 MiB)\n",
      "24/06/29 12:01:59 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:01:59 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[7] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:01:59 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:01:59 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7450 bytes) \n",
      "24/06/29 12:01:59 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:02:00 INFO CodeGenerator: Code generated in 11.490608 ms\n",
      "24/06/29 12:02:00 INFO JDBCRDD: closed connection\n",
      "24/06/29 12:02:00 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1927 bytes result sent to driver\n",
      "24/06/29 12:02:00 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 399 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:02:00 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:02:00 INFO DAGScheduler: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 0.514 s\n",
      "24/06/29 12:02:00 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/06/29 12:02:00 INFO DAGScheduler: running: Set()\n",
      "24/06/29 12:02:00 INFO DAGScheduler: waiting: Set()\n",
      "24/06/29 12:02:00 INFO DAGScheduler: failed: Set()\n",
      "24/06/29 12:02:00 INFO CodeGenerator: Code generated in 13.400411 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:02:00 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Got job 3 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Final stage: ResultStage 4 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:02:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.5 KiB, free 912.3 MiB)\n",
      "24/06/29 12:02:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 912.3 MiB)\n",
      "24/06/29 12:02:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.8:45173 (size: 5.9 KiB, free: 912.3 MiB)\n",
      "24/06/29 12:02:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:02:00 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:02:00 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (192.168.1.8, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/06/29 12:02:00 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)\n",
      "24/06/29 12:02:00 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/06/29 12:02:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms\n",
      "24/06/29 12:02:00 INFO CodeGenerator: Code generated in 9.119956 ms\n",
      "24/06/29 12:02:00 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 3995 bytes result sent to driver\n",
      "24/06/29 12:02:00 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 121 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:02:00 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:02:00 INFO DAGScheduler: ResultStage 4 (count at NativeMethodAccessorImpl.java:0) finished in 0.138 s\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:02:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Job 3 finished: count at NativeMethodAccessorImpl.java:0, took 0.159760 s\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Registering RDD 13 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Got map stage job 4 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Missing parents: List()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:02:00 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:02:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.8 KiB, free 912.2 MiB)\n",
      "24/06/29 12:02:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 912.2 MiB)\n",
      "24/06/29 12:02:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.8:45173 (size: 7.2 KiB, free: 912.3 MiB)\n",
      "24/06/29 12:02:00 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:02:00 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:02:00 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7450 bytes) \n",
      "24/06/29 12:02:00 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)\n",
      "24/06/29 12:02:00 INFO JDBCRDD: closed connection\n",
      "24/06/29 12:02:00 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 1884 bytes result sent to driver\n",
      "24/06/29 12:02:00 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 55 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:02:00 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:02:00 INFO DAGScheduler: ShuffleMapStage 5 (count at NativeMethodAccessorImpl.java:0) finished in 0.073 s\n",
      "24/06/29 12:02:00 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/06/29 12:02:00 INFO DAGScheduler: running: Set()\n",
      "24/06/29 12:02:00 INFO DAGScheduler: waiting: Set()\n",
      "24/06/29 12:02:00 INFO DAGScheduler: failed: Set()\n",
      "24/06/29 12:02:00 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Got job 5 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Final stage: ResultStage 7 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:02:00 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.5 KiB, free 912.2 MiB)\n",
      "24/06/29 12:02:00 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 912.2 MiB)\n",
      "24/06/29 12:02:00 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.8:45173 (size: 5.9 KiB, free: 912.3 MiB)\n",
      "24/06/29 12:02:00 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:02:00 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:02:00 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (192.168.1.8, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/06/29 12:02:00 INFO Executor: Running task 0.0 in stage 7.0 (TID 5)\n",
      "24/06/29 12:02:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.8:45173 in memory (size: 7.2 KiB, free: 912.3 MiB)\n",
      "24/06/29 12:02:00 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.8:45173 in memory (size: 7.2 KiB, free: 912.3 MiB)\n",
      "24/06/29 12:02:00 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/06/29 12:02:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "24/06/29 12:02:00 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.8:45173 in memory (size: 5.9 KiB, free: 912.3 MiB)\n",
      "24/06/29 12:02:00 INFO Executor: Finished task 0.0 in stage 7.0 (TID 5). 4038 bytes result sent to driver\n",
      "24/06/29 12:02:00 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 20 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:02:00 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:02:00 INFO DAGScheduler: ResultStage 7 (count at NativeMethodAccessorImpl.java:0) finished in 0.047 s\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:02:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "24/06/29 12:02:00 INFO DAGScheduler: Job 5 finished: count at NativeMethodAccessorImpl.java:0, took 0.052723 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:02:13 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:02:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:02:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:02:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:02:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:02:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:02:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:02:14 INFO CodeGenerator: Code generated in 39.687256 ms\n",
      "24/06/29 12:02:14 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.8:45173 in memory (size: 5.9 KiB, free: 912.3 MiB)\n",
      "24/06/29 12:02:14 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:02:14 INFO DAGScheduler: Got job 6 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:02:14 INFO DAGScheduler: Final stage: ResultStage 8 (save at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:02:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:02:14 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:02:14 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[19] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:02:14 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 214.9 KiB, free 912.1 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:02:14 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 77.1 KiB, free 912.0 MiB)\n",
      "24/06/29 12:02:14 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.8:45173 (size: 77.1 KiB, free: 912.2 MiB)\n",
      "24/06/29 12:02:14 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:02:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[19] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:02:14 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:02:14 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7461 bytes) \n",
      "24/06/29 12:02:14 INFO Executor: Running task 0.0 in stage 8.0 (TID 6)\n",
      "24/06/29 12:02:14 INFO CodeGenerator: Code generated in 16.086206 ms\n",
      "24/06/29 12:02:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:02:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:02:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:02:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:02:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:02:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:02:14 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:02:14 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:02:15 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/06/29 12:02:15 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"Date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Exchange\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"year\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"month\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"day\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary Date (STRING);\n",
      "  optional binary Currency (STRING);\n",
      "  optional double Exchange;\n",
      "  required binary year (STRING);\n",
      "  required binary month (STRING);\n",
      "  required binary day (STRING);\n",
      "}\n",
      "\n",
      "       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 1) / 1]\r",
      "24/06/29 12:02:15 INFO CodecPool: Got brand-new compressor [.snappy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:02:16 INFO JDBCRDD: closed connection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:02:17 INFO FileOutputCommitter: Saved output of task 'attempt_202406291202147618214460534605904_0008_m_000000_6' to hdfs://localhost:9000/lakehouse/LH_Global_Electronics_Retailer/Files/Bronze/exchange_rates/year=2024/month=06/day=29/exchange_rates_2024_06_29-version_8.parquet/_temporary/0/task_202406291202147618214460534605904_0008_m_000000\n",
      "24/06/29 12:02:17 INFO SparkHadoopMapRedUtil: attempt_202406291202147618214460534605904_0008_m_000000_6: Committed. Elapsed time: 27 ms.\n",
      "24/06/29 12:02:17 INFO Executor: Finished task 0.0 in stage 8.0 (TID 6). 2570 bytes result sent to driver\n",
      "24/06/29 12:02:17 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 2598 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:02:17 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:02:17 INFO DAGScheduler: ResultStage 8 (save at NativeMethodAccessorImpl.java:0) finished in 2.701 s\n",
      "24/06/29 12:02:17 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:02:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "24/06/29 12:02:17 INFO DAGScheduler: Job 6 finished: save at NativeMethodAccessorImpl.java:0, took 2.706347 s\n",
      "\r",
      "                                                                                \r",
      "24/06/29 12:02:17 INFO FileFormatWriter: Start to commit write Job 07c3c959-1a0a-42a1-8944-a767dd617461.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:02:17 INFO FileFormatWriter: Write Job 07c3c959-1a0a-42a1-8944-a767dd617461 committed. Elapsed time: 197 ms.\n",
      "24/06/29 12:02:17 INFO FileFormatWriter: Finished processing stats for write job 07c3c959-1a0a-42a1-8944-a767dd617461.\n",
      "24/06/29 12:02:17 INFO CodeGenerator: Code generated in 7.161371 ms\n",
      "24/06/29 12:02:17 INFO SparkContext: Starting job: collect at /tmp/ipykernel_62125/3616262314.py:82\n",
      "24/06/29 12:02:17 INFO DAGScheduler: Got job 7 (collect at /tmp/ipykernel_62125/3616262314.py:82) with 1 output partitions\n",
      "24/06/29 12:02:17 INFO DAGScheduler: Final stage: ResultStage 9 (collect at /tmp/ipykernel_62125/3616262314.py:82)\n",
      "24/06/29 12:02:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:02:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:02:17 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[21] at collect at /tmp/ipykernel_62125/3616262314.py:82), which has no missing parents\n",
      "24/06/29 12:02:17 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 7.4 KiB, free 912.0 MiB)\n",
      "24/06/29 12:02:17 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 912.0 MiB)\n",
      "24/06/29 12:02:17 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.8:45173 (size: 3.8 KiB, free: 912.2 MiB)\n",
      "24/06/29 12:02:17 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:02:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[21] at collect at /tmp/ipykernel_62125/3616262314.py:82) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:02:17 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:02:17 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7823 bytes) \n",
      "24/06/29 12:02:17 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)\n",
      "24/06/29 12:02:17 INFO CodeGenerator: Code generated in 5.840038 ms\n",
      "24/06/29 12:02:17 INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 1462 bytes result sent to driver\n",
      "24/06/29 12:02:17 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 15 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:02:17 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:02:17 INFO DAGScheduler: ResultStage 9 (collect at /tmp/ipykernel_62125/3616262314.py:82) finished in 0.024 s\n",
      "24/06/29 12:02:17 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:02:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "24/06/29 12:02:17 INFO DAGScheduler: Job 7 finished: collect at /tmp/ipykernel_62125/3616262314.py:82, took 0.027740 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task  5   Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:02:23 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:02:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:02:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:02:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:02:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:02:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:02:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:02:23 INFO CodeGenerator: Code generated in 21.855121 ms\n",
      "24/06/29 12:02:23 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:02:23 INFO DAGScheduler: Got job 8 (save at NativeMethodAccessorImpl.java:0) with 4 output partitions\n",
      "24/06/29 12:02:23 INFO DAGScheduler: Final stage: ResultStage 10 (save at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:02:23 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:02:23 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:02:23 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[28] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:02:23 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 220.5 KiB, free 911.8 MiB)\n",
      "24/06/29 12:02:23 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 911.7 MiB)\n",
      "24/06/29 12:02:23 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.8:45173 (size: 78.6 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:02:23 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:02:23 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 10 (MapPartitionsRDD[28] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "24/06/29 12:02:23 INFO TaskSchedulerImpl: Adding task set 10.0 with 4 tasks resource profile 0\n",
      "24/06/29 12:02:23 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/06/29 12:02:23 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 9) (192.168.1.8, executor driver, partition 1, PROCESS_LOCAL, 7595 bytes) \n",
      "24/06/29 12:02:23 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 10) (192.168.1.8, executor driver, partition 2, PROCESS_LOCAL, 7595 bytes) \n",
      "24/06/29 12:02:23 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 11) (192.168.1.8, executor driver, partition 3, PROCESS_LOCAL, 7827 bytes) \n",
      "24/06/29 12:02:23 INFO Executor: Running task 0.0 in stage 10.0 (TID 8)\n",
      "24/06/29 12:02:23 INFO Executor: Running task 1.0 in stage 10.0 (TID 9)\n",
      "24/06/29 12:02:23 INFO Executor: Running task 2.0 in stage 10.0 (TID 10)\n",
      "24/06/29 12:02:23 INFO Executor: Running task 3.0 in stage 10.0 (TID 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:02:32 INFO CodeGenerator: Code generated in 20.776797 ms\n",
      "24/06/29 12:02:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:02:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:02:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:02:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:02:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:02:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:02:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:02:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:02:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:02:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:02:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:02:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:02:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:02:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:02:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:02:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:02:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:02:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:02:32 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:02:32 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:02:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:02:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:02:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:02:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:02:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:02:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:02:32 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/06/29 12:02:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"BatchId\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TaskName\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceConnection\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceDatabase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TargetTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"StartTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"EndTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceRowsRead\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetInserted\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetUpdated\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnMissing\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnNull\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Error\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Phase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional int32 BatchId;\n",
      "  optional binary TaskName (STRING);\n",
      "  optional binary SourceConnection (STRING);\n",
      "  optional binary SourceDatabase (STRING);\n",
      "  optional binary SourceTable (STRING);\n",
      "  optional binary TargetTable (STRING);\n",
      "  optional binary StartTime (STRING);\n",
      "  optional binary EndTime (STRING);\n",
      "  optional int32 SourceRowsRead;\n",
      "  optional int32 NumTargetInserted;\n",
      "  optional int32 NumTargetUpdated;\n",
      "  optional binary Status (STRING);\n",
      "  optional binary ColumnMissing (STRING);\n",
      "  optional binary ColumnNull (STRING);\n",
      "  optional binary Error (STRING);\n",
      "  optional binary Phase (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/06/29 12:02:32 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.1.8:45173 in memory (size: 3.8 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:02:32 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.1.8:45173 in memory (size: 77.1 KiB, free: 912.2 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:02:33 INFO PythonRunner: Times: total = 9030, boot = 8613, init = 417, finish = 0\n",
      "24/06/29 12:02:33 INFO PythonRunner: Times: total = 9032, boot = 8619, init = 413, finish = 0\n",
      "24/06/29 12:02:33 INFO PythonRunner: Times: total = 9032, boot = 8609, init = 423, finish = 0\n",
      "24/06/29 12:02:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202406291202231824280842881296473_0010_m_000001_9\n",
      "24/06/29 12:02:33 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202406291202231824280842881296473_0010_m_000002_10\n",
      "24/06/29 12:02:33 INFO Executor: Finished task 2.0 in stage 10.0 (TID 10). 2832 bytes result sent to driver\n",
      "24/06/29 12:02:33 INFO Executor: Finished task 1.0 in stage 10.0 (TID 9). 2832 bytes result sent to driver\n",
      "24/06/29 12:02:33 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 10) in 9157 ms on 192.168.1.8 (executor driver) (1/4)\n",
      "24/06/29 12:02:33 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 9) in 9160 ms on 192.168.1.8 (executor driver) (2/4)\n",
      "24/06/29 12:02:33 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 52941\n",
      "24/06/29 12:02:33 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:02:33 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:02:33 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/06/29 12:02:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"BatchId\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TaskName\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceConnection\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceDatabase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TargetTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"StartTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"EndTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceRowsRead\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetInserted\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetUpdated\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnMissing\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnNull\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Error\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Phase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional int32 BatchId;\n",
      "  optional binary TaskName (STRING);\n",
      "  optional binary SourceConnection (STRING);\n",
      "  optional binary SourceDatabase (STRING);\n",
      "  optional binary SourceTable (STRING);\n",
      "  optional binary TargetTable (STRING);\n",
      "  optional binary StartTime (STRING);\n",
      "  optional binary EndTime (STRING);\n",
      "  optional int32 SourceRowsRead;\n",
      "  optional int32 NumTargetInserted;\n",
      "  optional int32 NumTargetUpdated;\n",
      "  optional binary Status (STRING);\n",
      "  optional binary ColumnMissing (STRING);\n",
      "  optional binary ColumnNull (STRING);\n",
      "  optional binary Error (STRING);\n",
      "  optional binary Phase (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/06/29 12:02:33 INFO PythonRunner: Times: total = 9042, boot = 8605, init = 437, finish = 0\n",
      "24/06/29 12:02:33 INFO FileOutputCommitter: Saved output of task 'attempt_202406291202231824280842881296473_0010_m_000000_8' to hdfs://localhost:9000/lakehouse/LH_Global_Electronics_Retailer/Files/log/2024-06-29/batch_8/_temporary/0/task_202406291202231824280842881296473_0010_m_000000\n",
      "24/06/29 12:02:33 INFO SparkHadoopMapRedUtil: attempt_202406291202231824280842881296473_0010_m_000000_8: Committed. Elapsed time: 27 ms.\n",
      "24/06/29 12:02:33 INFO Executor: Finished task 0.0 in stage 10.0 (TID 8). 2875 bytes result sent to driver\n",
      "24/06/29 12:02:33 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 9295 ms on 192.168.1.8 (executor driver) (3/4)\n",
      "\r",
      "[Stage 10:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:02:33 INFO FileOutputCommitter: Saved output of task 'attempt_202406291202231824280842881296473_0010_m_000003_11' to hdfs://localhost:9000/lakehouse/LH_Global_Electronics_Retailer/Files/log/2024-06-29/batch_8/_temporary/0/task_202406291202231824280842881296473_0010_m_000003\n",
      "24/06/29 12:02:33 INFO SparkHadoopMapRedUtil: attempt_202406291202231824280842881296473_0010_m_000003_11: Committed. Elapsed time: 8 ms.\n",
      "24/06/29 12:02:33 INFO Executor: Finished task 3.0 in stage 10.0 (TID 11). 2918 bytes result sent to driver\n",
      "24/06/29 12:02:33 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 11) in 9399 ms on 192.168.1.8 (executor driver) (4/4)\n",
      "24/06/29 12:02:33 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:02:33 INFO DAGScheduler: ResultStage 10 (save at NativeMethodAccessorImpl.java:0) finished in 9.458 s\n",
      "24/06/29 12:02:33 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:02:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "24/06/29 12:02:33 INFO DAGScheduler: Job 8 finished: save at NativeMethodAccessorImpl.java:0, took 9.470157 s\n",
      "\r",
      "                                                                                \r",
      "24/06/29 12:02:33 INFO FileFormatWriter: Start to commit write Job 0ed40e76-7670-4411-b2d6-9843c1bab2ba.\n",
      "24/06/29 12:02:33 INFO FileFormatWriter: Write Job 0ed40e76-7670-4411-b2d6-9843c1bab2ba committed. Elapsed time: 60 ms.\n",
      "24/06/29 12:02:33 INFO FileFormatWriter: Finished processing stats for write job 0ed40e76-7670-4411-b2d6-9843c1bab2ba.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:02:33 INFO CodeGenerator: Code generated in 7.273723 ms\n",
      "24/06/29 12:02:33 INFO SparkContext: Starting job: collect at /tmp/ipykernel_62125/3616262314.py:43\n",
      "24/06/29 12:02:33 INFO DAGScheduler: Got job 9 (collect at /tmp/ipykernel_62125/3616262314.py:43) with 1 output partitions\n",
      "24/06/29 12:02:33 INFO DAGScheduler: Final stage: ResultStage 11 (collect at /tmp/ipykernel_62125/3616262314.py:43)\n",
      "24/06/29 12:02:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:02:33 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:02:33 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[30] at collect at /tmp/ipykernel_62125/3616262314.py:43), which has no missing parents\n",
      "24/06/29 12:02:33 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 7.4 KiB, free 912.0 MiB)\n",
      "24/06/29 12:02:33 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 912.0 MiB)\n",
      "24/06/29 12:02:33 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.8:45173 (size: 3.8 KiB, free: 912.2 MiB)\n",
      "24/06/29 12:02:33 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:02:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[30] at collect at /tmp/ipykernel_62125/3616262314.py:43) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:02:33 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:02:33 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 12) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7823 bytes) \n",
      "24/06/29 12:02:33 INFO Executor: Running task 0.0 in stage 11.0 (TID 12)\n",
      "24/06/29 12:02:33 INFO CodeGenerator: Code generated in 8.625488 ms\n",
      "24/06/29 12:02:33 INFO Executor: Finished task 0.0 in stage 11.0 (TID 12). 1462 bytes result sent to driver\n",
      "24/06/29 12:02:33 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 12) in 16 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:02:33 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:02:33 INFO DAGScheduler: ResultStage 11 (collect at /tmp/ipykernel_62125/3616262314.py:43) finished in 0.022 s\n",
      "24/06/29 12:02:33 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:02:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
      "24/06/29 12:02:33 INFO DAGScheduler: Job 9 finished: collect at /tmp/ipykernel_62125/3616262314.py:43, took 0.027361 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:02:33 INFO DAGScheduler: Registering RDD 33 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "24/06/29 12:02:33 INFO DAGScheduler: Got map stage job 10 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:02:33 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:02:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:02:33 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:02:33 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[33] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:02:33 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 13.8 KiB, free 912.0 MiB)\n",
      "24/06/29 12:02:33 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 912.0 MiB)\n",
      "24/06/29 12:02:34 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.1.8:45173 (size: 7.2 KiB, free: 912.2 MiB)\n",
      "24/06/29 12:02:34 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[33] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:02:34 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:02:34 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 13) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7450 bytes) \n",
      "24/06/29 12:02:34 INFO Executor: Running task 0.0 in stage 12.0 (TID 13)\n",
      "24/06/29 12:02:34 INFO JDBCRDD: closed connection\n",
      "24/06/29 12:02:34 INFO Executor: Finished task 0.0 in stage 12.0 (TID 13). 1884 bytes result sent to driver\n",
      "24/06/29 12:02:34 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 13) in 115 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:02:34 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:02:34 INFO DAGScheduler: ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0) finished in 0.141 s\n",
      "24/06/29 12:02:34 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/06/29 12:02:34 INFO DAGScheduler: running: Set()\n",
      "24/06/29 12:02:34 INFO DAGScheduler: waiting: Set()\n",
      "24/06/29 12:02:34 INFO DAGScheduler: failed: Set()\n",
      "24/06/29 12:02:34 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Got job 11 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Final stage: ResultStage 14 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[36] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:02:34 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 12.5 KiB, free 912.0 MiB)\n",
      "24/06/29 12:02:34 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 911.9 MiB)\n",
      "24/06/29 12:02:34 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.1.8:45173 (size: 5.9 KiB, free: 912.2 MiB)\n",
      "24/06/29 12:02:34 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[36] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:02:34 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:02:34 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (192.168.1.8, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/06/29 12:02:34 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)\n",
      "24/06/29 12:02:34 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/06/29 12:02:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/06/29 12:02:34 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 3995 bytes result sent to driver\n",
      "24/06/29 12:02:34 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 7 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:02:34 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:02:34 INFO DAGScheduler: ResultStage 14 (count at NativeMethodAccessorImpl.java:0) finished in 0.015 s\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:02:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Job 11 finished: count at NativeMethodAccessorImpl.java:0, took 0.018764 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:02:34 INFO DAGScheduler: Registering RDD 39 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Got map stage job 12 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[39] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:02:34 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 13.8 KiB, free 911.9 MiB)\n",
      "24/06/29 12:02:34 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 911.9 MiB)\n",
      "24/06/29 12:02:34 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.1.8:45173 (size: 7.2 KiB, free: 912.2 MiB)\n",
      "24/06/29 12:02:34 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[39] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:02:34 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:02:34 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7450 bytes) \n",
      "24/06/29 12:02:34 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)\n",
      "24/06/29 12:02:34 INFO JDBCRDD: closed connection\n",
      "24/06/29 12:02:34 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 1884 bytes result sent to driver\n",
      "24/06/29 12:02:34 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 101 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:02:34 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:02:34 INFO DAGScheduler: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0) finished in 0.107 s\n",
      "24/06/29 12:02:34 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/06/29 12:02:34 INFO DAGScheduler: running: Set()\n",
      "24/06/29 12:02:34 INFO DAGScheduler: waiting: Set()\n",
      "24/06/29 12:02:34 INFO DAGScheduler: failed: Set()\n",
      "24/06/29 12:02:34 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Got job 13 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Final stage: ResultStage 17 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[42] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:02:34 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 12.5 KiB, free 911.9 MiB)\n",
      "24/06/29 12:02:34 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 911.9 MiB)\n",
      "24/06/29 12:02:34 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.1.8:45173 (size: 5.9 KiB, free: 912.2 MiB)\n",
      "24/06/29 12:02:34 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[42] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:02:34 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:02:34 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 16) (192.168.1.8, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/06/29 12:02:34 INFO Executor: Running task 0.0 in stage 17.0 (TID 16)\n",
      "24/06/29 12:02:34 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/06/29 12:02:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/06/29 12:02:34 INFO Executor: Finished task 0.0 in stage 17.0 (TID 16). 3995 bytes result sent to driver\n",
      "24/06/29 12:02:34 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 16) in 9 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:02:34 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:02:34 INFO DAGScheduler: ResultStage 17 (count at NativeMethodAccessorImpl.java:0) finished in 0.015 s\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:02:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished\n",
      "24/06/29 12:02:34 INFO DAGScheduler: Job 13 finished: count at NativeMethodAccessorImpl.java:0, took 0.019251 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:04:08 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:04:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:04:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:04:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:04:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:04:08 INFO CodeGenerator: Code generated in 14.551554 ms\n",
      "24/06/29 12:04:08 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:04:08 INFO DAGScheduler: Got job 14 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:04:08 INFO DAGScheduler: Final stage: ResultStage 18 (save at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:04:08 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:04:08 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:04:08 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[45] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:04:08 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 218.1 KiB, free 911.7 MiB)\n",
      "24/06/29 12:04:09 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 77.9 KiB, free 911.6 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:04:09 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.1.8:45173 (size: 77.9 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:04:09 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:04:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[45] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:04:09 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:04:09 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 17) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7461 bytes) \n",
      "24/06/29 12:04:09 INFO Executor: Running task 0.0 in stage 18.0 (TID 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:04:14 INFO CodeGenerator: Code generated in 18.8267 ms\n",
      "24/06/29 12:04:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:04:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:04:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:04:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:04:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:14 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:04:14 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:04:14 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/06/29 12:04:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"ProductKey\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Product Name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Brand\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Color\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Unit Cost USD\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Unit Price USD\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"SubcategoryKey\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Subcategory\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"CategoryKey\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Category\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"year\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"month\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"day\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional int32 ProductKey;\n",
      "  optional binary Product Name (STRING);\n",
      "  optional binary Brand (STRING);\n",
      "  optional binary Color (STRING);\n",
      "  optional binary Unit Cost USD (STRING);\n",
      "  optional binary Unit Price USD (STRING);\n",
      "  optional binary SubcategoryKey (STRING);\n",
      "  optional binary Subcategory (STRING);\n",
      "  optional binary CategoryKey (STRING);\n",
      "  optional binary Category (STRING);\n",
      "  required binary year (STRING);\n",
      "  required binary month (STRING);\n",
      "  required binary day (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/06/29 12:04:14 INFO JDBCRDD: closed connection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:04:14 INFO FileOutputCommitter: Saved output of task 'attempt_202406291204089132455393483732179_0018_m_000000_17' to hdfs://localhost:9000/lakehouse/LH_Global_Electronics_Retailer/Files/Bronze/products/year=2024/month=06/day=29/products_2024_06_29-version_8.parquet/_temporary/0/task_202406291204089132455393483732179_0018_m_000000\n",
      "24/06/29 12:04:14 INFO SparkHadoopMapRedUtil: attempt_202406291204089132455393483732179_0018_m_000000_17: Committed. Elapsed time: 29 ms.\n",
      "24/06/29 12:04:14 INFO Executor: Finished task 0.0 in stage 18.0 (TID 17). 2484 bytes result sent to driver\n",
      "24/06/29 12:04:14 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 17) in 5704 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:04:14 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:04:14 INFO DAGScheduler: ResultStage 18 (save at NativeMethodAccessorImpl.java:0) finished in 6.038 s\n",
      "24/06/29 12:04:14 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:04:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
      "24/06/29 12:04:14 INFO DAGScheduler: Job 14 finished: save at NativeMethodAccessorImpl.java:0, took 6.043768 s\n",
      "\r",
      "                                                                                \r",
      "24/06/29 12:04:14 INFO FileFormatWriter: Start to commit write Job ba643b00-699c-443b-9240-54d372dab6d1.\n",
      "24/06/29 12:04:15 INFO FileFormatWriter: Write Job ba643b00-699c-443b-9240-54d372dab6d1 committed. Elapsed time: 72 ms.\n",
      "24/06/29 12:04:15 INFO FileFormatWriter: Finished processing stats for write job ba643b00-699c-443b-9240-54d372dab6d1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task  3   Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:04:15 INFO CodeGenerator: Code generated in 3.659765 ms\n",
      "24/06/29 12:04:15 INFO SparkContext: Starting job: collect at /tmp/ipykernel_62125/3616262314.py:82\n",
      "24/06/29 12:04:15 INFO DAGScheduler: Got job 15 (collect at /tmp/ipykernel_62125/3616262314.py:82) with 1 output partitions\n",
      "24/06/29 12:04:15 INFO DAGScheduler: Final stage: ResultStage 19 (collect at /tmp/ipykernel_62125/3616262314.py:82)\n",
      "24/06/29 12:04:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:04:15 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:04:15 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[47] at collect at /tmp/ipykernel_62125/3616262314.py:82), which has no missing parents\n",
      "24/06/29 12:04:15 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 7.4 KiB, free 911.6 MiB)\n",
      "24/06/29 12:04:15 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 911.6 MiB)\n",
      "24/06/29 12:04:15 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.1.8:45173 (size: 3.8 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:04:15 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:04:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[47] at collect at /tmp/ipykernel_62125/3616262314.py:82) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:04:15 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:04:15 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 18) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7823 bytes) \n",
      "24/06/29 12:04:15 INFO Executor: Running task 0.0 in stage 19.0 (TID 18)\n",
      "24/06/29 12:04:15 INFO CodeGenerator: Code generated in 8.037328 ms\n",
      "24/06/29 12:04:15 INFO Executor: Finished task 0.0 in stage 19.0 (TID 18). 1462 bytes result sent to driver\n",
      "24/06/29 12:04:15 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 18) in 15 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:04:15 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:04:15 INFO DAGScheduler: ResultStage 19 (collect at /tmp/ipykernel_62125/3616262314.py:82) finished in 0.067 s\n",
      "24/06/29 12:04:15 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:04:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
      "24/06/29 12:04:15 INFO DAGScheduler: Job 15 finished: collect at /tmp/ipykernel_62125/3616262314.py:82, took 0.070380 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:04:15 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:04:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:04:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:04:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:04:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:15 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:04:15 INFO DAGScheduler: Got job 16 (save at NativeMethodAccessorImpl.java:0) with 4 output partitions\n",
      "24/06/29 12:04:15 INFO DAGScheduler: Final stage: ResultStage 20 (save at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:04:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:04:15 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:04:15 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[54] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:04:15 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 220.5 KiB, free 911.4 MiB)\n",
      "24/06/29 12:04:15 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 78.7 KiB, free 911.3 MiB)\n",
      "24/06/29 12:04:15 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.1.8:45173 (size: 78.7 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:04:15 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:04:15 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 20 (MapPartitionsRDD[54] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "24/06/29 12:04:15 INFO TaskSchedulerImpl: Adding task set 20.0 with 4 tasks resource profile 0\n",
      "24/06/29 12:04:15 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 19) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/06/29 12:04:15 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 20) (192.168.1.8, executor driver, partition 1, PROCESS_LOCAL, 7595 bytes) \n",
      "24/06/29 12:04:15 INFO TaskSetManager: Starting task 2.0 in stage 20.0 (TID 21) (192.168.1.8, executor driver, partition 2, PROCESS_LOCAL, 7595 bytes) \n",
      "24/06/29 12:04:15 INFO TaskSetManager: Starting task 3.0 in stage 20.0 (TID 22) (192.168.1.8, executor driver, partition 3, PROCESS_LOCAL, 7815 bytes) \n",
      "24/06/29 12:04:15 INFO Executor: Running task 0.0 in stage 20.0 (TID 19)\n",
      "24/06/29 12:04:15 INFO Executor: Running task 1.0 in stage 20.0 (TID 20)\n",
      "24/06/29 12:04:15 INFO Executor: Running task 2.0 in stage 20.0 (TID 21)\n",
      "24/06/29 12:04:15 INFO Executor: Running task 3.0 in stage 20.0 (TID 22)\n",
      "24/06/29 12:04:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:04:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:04:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:04:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:04:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:04:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:04:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:04:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:04:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:16 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:04:16 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:04:16 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/06/29 12:04:16 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 192.168.1.8:45173 in memory (size: 77.9 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:04:16 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"BatchId\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TaskName\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceConnection\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceDatabase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TargetTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"StartTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"EndTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceRowsRead\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetInserted\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetUpdated\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnMissing\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnNull\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Error\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Phase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional int32 BatchId;\n",
      "  optional binary TaskName (STRING);\n",
      "  optional binary SourceConnection (STRING);\n",
      "  optional binary SourceDatabase (STRING);\n",
      "  optional binary SourceTable (STRING);\n",
      "  optional binary TargetTable (STRING);\n",
      "  optional binary StartTime (STRING);\n",
      "  optional binary EndTime (STRING);\n",
      "  optional int32 SourceRowsRead;\n",
      "  optional int32 NumTargetInserted;\n",
      "  optional int32 NumTargetUpdated;\n",
      "  optional binary Status (STRING);\n",
      "  optional binary ColumnMissing (STRING);\n",
      "  optional binary ColumnNull (STRING);\n",
      "  optional binary Error (STRING);\n",
      "  optional binary Phase (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/06/29 12:04:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:04:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:04:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:04:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:04:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:04:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:04:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:04:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:04:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:04:16 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.1.8:45173 in memory (size: 7.2 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:04:16 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.1.8:45173 in memory (size: 5.9 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:04:16 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.1.8:45173 in memory (size: 3.8 KiB, free: 912.1 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:04:16 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.1.8:45173 in memory (size: 78.6 KiB, free: 912.2 MiB)\n",
      "24/06/29 12:04:16 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 192.168.1.8:45173 in memory (size: 3.8 KiB, free: 912.2 MiB)\n",
      "24/06/29 12:04:16 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 192.168.1.8:45173 in memory (size: 5.9 KiB, free: 912.2 MiB)\n",
      "24/06/29 12:04:16 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.1.8:45173 in memory (size: 7.2 KiB, free: 912.2 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:04:16 INFO PythonRunner: Times: total = 784, boot = 82, init = 702, finish = 0\n",
      "24/06/29 12:04:16 INFO PythonRunner: Times: total = 784, boot = 92, init = 692, finish = 0\n",
      "24/06/29 12:04:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202406291204158767291194395004065_0020_m_000001_20\n",
      "24/06/29 12:04:16 INFO Executor: Finished task 1.0 in stage 20.0 (TID 20). 2832 bytes result sent to driver\n",
      "24/06/29 12:04:16 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:04:16 INFO PythonRunner: Times: total = 794, boot = 69, init = 725, finish = 0\n",
      "24/06/29 12:04:16 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:04:16 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 20) in 806 ms on 192.168.1.8 (executor driver) (1/4)\n",
      "24/06/29 12:04:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202406291204158767291194395004065_0020_m_000002_21\n",
      "24/06/29 12:04:16 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/06/29 12:04:16 INFO Executor: Finished task 2.0 in stage 20.0 (TID 21). 2832 bytes result sent to driver\n",
      "24/06/29 12:04:16 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"BatchId\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TaskName\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceConnection\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceDatabase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TargetTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"StartTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"EndTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceRowsRead\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetInserted\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetUpdated\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnMissing\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnNull\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Error\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Phase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional int32 BatchId;\n",
      "  optional binary TaskName (STRING);\n",
      "  optional binary SourceConnection (STRING);\n",
      "  optional binary SourceDatabase (STRING);\n",
      "  optional binary SourceTable (STRING);\n",
      "  optional binary TargetTable (STRING);\n",
      "  optional binary StartTime (STRING);\n",
      "  optional binary EndTime (STRING);\n",
      "  optional int32 SourceRowsRead;\n",
      "  optional int32 NumTargetInserted;\n",
      "  optional int32 NumTargetUpdated;\n",
      "  optional binary Status (STRING);\n",
      "  optional binary ColumnMissing (STRING);\n",
      "  optional binary ColumnNull (STRING);\n",
      "  optional binary Error (STRING);\n",
      "  optional binary Phase (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/06/29 12:04:16 INFO TaskSetManager: Finished task 2.0 in stage 20.0 (TID 21) in 811 ms on 192.168.1.8 (executor driver) (2/4)\n",
      "24/06/29 12:04:16 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "24/06/29 12:04:16 INFO PythonRunner: Times: total = 791, boot = 87, init = 704, finish = 0\n",
      "24/06/29 12:04:16 INFO FileOutputCommitter: Saved output of task 'attempt_202406291204158767291194395004065_0020_m_000000_19' to hdfs://localhost:9000/lakehouse/LH_Global_Electronics_Retailer/Files/log/2024-06-29/batch_8/_temporary/0/task_202406291204158767291194395004065_0020_m_000000\n",
      "24/06/29 12:04:16 INFO SparkHadoopMapRedUtil: attempt_202406291204158767291194395004065_0020_m_000000_19: Committed. Elapsed time: 85 ms.\n",
      "24/06/29 12:04:16 INFO Executor: Finished task 0.0 in stage 20.0 (TID 19). 2875 bytes result sent to driver\n",
      "24/06/29 12:04:16 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 19) in 968 ms on 192.168.1.8 (executor driver) (3/4)\n",
      "\r",
      "[Stage 20:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:04:16 INFO FileOutputCommitter: Saved output of task 'attempt_202406291204158767291194395004065_0020_m_000003_22' to hdfs://localhost:9000/lakehouse/LH_Global_Electronics_Retailer/Files/log/2024-06-29/batch_8/_temporary/0/task_202406291204158767291194395004065_0020_m_000003\n",
      "24/06/29 12:04:16 INFO SparkHadoopMapRedUtil: attempt_202406291204158767291194395004065_0020_m_000003_22: Committed. Elapsed time: 51 ms.\n",
      "24/06/29 12:04:16 INFO Executor: Finished task 3.0 in stage 20.0 (TID 22). 2918 bytes result sent to driver\n",
      "24/06/29 12:04:16 INFO TaskSetManager: Finished task 3.0 in stage 20.0 (TID 22) in 1067 ms on 192.168.1.8 (executor driver) (4/4)\n",
      "24/06/29 12:04:16 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:04:16 INFO DAGScheduler: ResultStage 20 (save at NativeMethodAccessorImpl.java:0) finished in 1.084 s\n",
      "24/06/29 12:04:16 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:04:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished\n",
      "24/06/29 12:04:16 INFO DAGScheduler: Job 16 finished: save at NativeMethodAccessorImpl.java:0, took 1.086528 s\n",
      "\r",
      "                                                                                \r",
      "24/06/29 12:04:16 INFO FileFormatWriter: Start to commit write Job f6285ed6-7cf7-4725-9e04-455ab4af627f.\n",
      "24/06/29 12:04:17 INFO FileFormatWriter: Write Job f6285ed6-7cf7-4725-9e04-455ab4af627f committed. Elapsed time: 137 ms.\n",
      "24/06/29 12:04:17 INFO FileFormatWriter: Finished processing stats for write job f6285ed6-7cf7-4725-9e04-455ab4af627f.\n",
      "24/06/29 12:04:17 INFO CodeGenerator: Code generated in 3.992378 ms\n",
      "24/06/29 12:04:17 INFO SparkContext: Starting job: collect at /tmp/ipykernel_62125/3616262314.py:43\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Got job 17 (collect at /tmp/ipykernel_62125/3616262314.py:43) with 1 output partitions\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Final stage: ResultStage 21 (collect at /tmp/ipykernel_62125/3616262314.py:43)\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[56] at collect at /tmp/ipykernel_62125/3616262314.py:43), which has no missing parents\n",
      "24/06/29 12:04:17 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 7.4 KiB, free 912.0 MiB)\n",
      "24/06/29 12:04:17 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 912.0 MiB)\n",
      "24/06/29 12:04:17 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 192.168.1.8:45173 (size: 3.8 KiB, free: 912.2 MiB)\n",
      "24/06/29 12:04:17 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[56] at collect at /tmp/ipykernel_62125/3616262314.py:43) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:04:17 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:04:17 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 23) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7823 bytes) \n",
      "24/06/29 12:04:17 INFO Executor: Running task 0.0 in stage 21.0 (TID 23)\n",
      "24/06/29 12:04:17 INFO CodeGenerator: Code generated in 3.779292 ms\n",
      "24/06/29 12:04:17 INFO Executor: Finished task 0.0 in stage 21.0 (TID 23). 1419 bytes result sent to driver\n",
      "24/06/29 12:04:17 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 23) in 7 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:04:17 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:04:17 INFO DAGScheduler: ResultStage 21 (collect at /tmp/ipykernel_62125/3616262314.py:43) finished in 0.011 s\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:04:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Job 17 finished: collect at /tmp/ipykernel_62125/3616262314.py:43, took 0.012979 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:04:17 INFO DAGScheduler: Registering RDD 59 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 4\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Got map stage job 18 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:04:17 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 13.8 KiB, free 912.0 MiB)\n",
      "24/06/29 12:04:17 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 912.0 MiB)\n",
      "24/06/29 12:04:17 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 192.168.1.8:45173 (size: 7.2 KiB, free: 912.2 MiB)\n",
      "24/06/29 12:04:17 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[59] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:04:17 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:04:17 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 24) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7450 bytes) \n",
      "24/06/29 12:04:17 INFO Executor: Running task 0.0 in stage 22.0 (TID 24)\n",
      "24/06/29 12:04:17 INFO JDBCRDD: closed connection\n",
      "24/06/29 12:04:17 INFO Executor: Finished task 0.0 in stage 22.0 (TID 24). 1884 bytes result sent to driver\n",
      "24/06/29 12:04:17 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 24) in 71 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:04:17 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:04:17 INFO DAGScheduler: ShuffleMapStage 22 (count at NativeMethodAccessorImpl.java:0) finished in 0.075 s\n",
      "24/06/29 12:04:17 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/06/29 12:04:17 INFO DAGScheduler: running: Set()\n",
      "24/06/29 12:04:17 INFO DAGScheduler: waiting: Set()\n",
      "24/06/29 12:04:17 INFO DAGScheduler: failed: Set()\n",
      "24/06/29 12:04:17 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Got job 19 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Final stage: ResultStage 24 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[62] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:04:17 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 12.5 KiB, free 912.0 MiB)\n",
      "24/06/29 12:04:17 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 911.9 MiB)\n",
      "24/06/29 12:04:17 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 192.168.1.8:45173 (size: 5.9 KiB, free: 912.2 MiB)\n",
      "24/06/29 12:04:17 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[62] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:04:17 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:04:17 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 25) (192.168.1.8, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/06/29 12:04:17 INFO Executor: Running task 0.0 in stage 24.0 (TID 25)\n",
      "24/06/29 12:04:17 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/06/29 12:04:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/06/29 12:04:17 INFO Executor: Finished task 0.0 in stage 24.0 (TID 25). 3995 bytes result sent to driver\n",
      "24/06/29 12:04:17 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 25) in 4 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:04:17 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:04:17 INFO DAGScheduler: ResultStage 24 (count at NativeMethodAccessorImpl.java:0) finished in 0.008 s\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:04:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Job 19 finished: count at NativeMethodAccessorImpl.java:0, took 0.009144 s\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Registering RDD 65 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 5\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Got map stage job 20 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Final stage: ShuffleMapStage 25 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[65] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:04:17 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 13.8 KiB, free 911.9 MiB)\n",
      "24/06/29 12:04:17 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 911.9 MiB)\n",
      "24/06/29 12:04:17 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 192.168.1.8:45173 (size: 7.2 KiB, free: 912.2 MiB)\n",
      "24/06/29 12:04:17 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[65] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:04:17 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:04:17 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 26) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7450 bytes) \n",
      "24/06/29 12:04:17 INFO Executor: Running task 0.0 in stage 25.0 (TID 26)\n",
      "24/06/29 12:04:17 INFO JDBCRDD: closed connection\n",
      "24/06/29 12:04:17 INFO Executor: Finished task 0.0 in stage 25.0 (TID 26). 1884 bytes result sent to driver\n",
      "24/06/29 12:04:17 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 26) in 70 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:04:17 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:04:17 INFO DAGScheduler: ShuffleMapStage 25 (count at NativeMethodAccessorImpl.java:0) finished in 0.075 s\n",
      "24/06/29 12:04:17 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/06/29 12:04:17 INFO DAGScheduler: running: Set()\n",
      "24/06/29 12:04:17 INFO DAGScheduler: waiting: Set()\n",
      "24/06/29 12:04:17 INFO DAGScheduler: failed: Set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:04:17 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Got job 21 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Final stage: ResultStage 27 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[68] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:04:17 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 12.5 KiB, free 911.9 MiB)\n",
      "24/06/29 12:04:17 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 911.9 MiB)\n",
      "24/06/29 12:04:17 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 192.168.1.8:45173 (size: 5.9 KiB, free: 912.2 MiB)\n",
      "24/06/29 12:04:17 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[68] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:04:17 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:04:17 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 27) (192.168.1.8, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/06/29 12:04:17 INFO Executor: Running task 0.0 in stage 27.0 (TID 27)\n",
      "24/06/29 12:04:17 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/06/29 12:04:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/06/29 12:04:17 INFO Executor: Finished task 0.0 in stage 27.0 (TID 27). 3995 bytes result sent to driver\n",
      "24/06/29 12:04:17 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 27) in 7 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:04:17 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:04:17 INFO DAGScheduler: ResultStage 27 (count at NativeMethodAccessorImpl.java:0) finished in 0.010 s\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:04:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished\n",
      "24/06/29 12:04:17 INFO DAGScheduler: Job 21 finished: count at NativeMethodAccessorImpl.java:0, took 0.013522 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:04:58 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:04:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:04:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:04:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:04:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:58 INFO CodeGenerator: Code generated in 9.50874 ms\n",
      "24/06/29 12:04:58 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:04:58 INFO DAGScheduler: Got job 22 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:04:58 INFO DAGScheduler: Final stage: ResultStage 28 (save at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:04:58 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:04:58 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:04:58 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[71] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:04:58 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 217.6 KiB, free 911.7 MiB)\n",
      "24/06/29 12:04:58 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 77.8 KiB, free 911.6 MiB)\n",
      "24/06/29 12:04:58 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 192.168.1.8:45173 (size: 77.8 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:04:58 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:04:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[71] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:04:58 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:04:58 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 28) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7461 bytes) \n",
      "24/06/29 12:04:58 INFO Executor: Running task 0.0 in stage 28.0 (TID 28)\n",
      "24/06/29 12:04:58 INFO CodeGenerator: Code generated in 11.029696 ms\n",
      "24/06/29 12:04:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:04:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:04:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:04:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:04:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:04:58 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:04:58 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:04:58 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/06/29 12:04:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"Order Number\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Line Item\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Order Date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Delivery Date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"CustomerKey\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"StoreKey\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"ProductKey\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Quantity\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Currency Code\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"year\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"month\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"day\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional int32 Order Number;\n",
      "  optional int32 Line Item;\n",
      "  optional binary Order Date (STRING);\n",
      "  optional binary Delivery Date (STRING);\n",
      "  optional int32 CustomerKey;\n",
      "  optional int32 StoreKey;\n",
      "  optional int32 ProductKey;\n",
      "  optional int32 Quantity;\n",
      "  optional binary Currency Code (STRING);\n",
      "  required binary year (STRING);\n",
      "  required binary month (STRING);\n",
      "  required binary day (STRING);\n",
      "}\n",
      "\n",
      "       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:04:58 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 192.168.1.8:45173 in memory (size: 3.8 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:04:58 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 192.168.1.8:45173 in memory (size: 7.2 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:04:58 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 192.168.1.8:45173 in memory (size: 5.9 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:04:58 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 192.168.1.8:45173 in memory (size: 5.9 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:04:58 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 192.168.1.8:45173 in memory (size: 7.2 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:04:59 INFO JDBCRDD: closed connection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:00 INFO FileOutputCommitter: Saved output of task 'attempt_202406291204584401476677628623359_0028_m_000000_28' to hdfs://localhost:9000/lakehouse/LH_Global_Electronics_Retailer/Files/Bronze/sales/year=2024/month=06/day=29/sales_2024_06_29-version_8.parquet/_temporary/0/task_202406291204584401476677628623359_0028_m_000000\n",
      "24/06/29 12:05:00 INFO SparkHadoopMapRedUtil: attempt_202406291204584401476677628623359_0028_m_000000_28: Committed. Elapsed time: 218 ms.\n",
      "24/06/29 12:05:00 INFO Executor: Finished task 0.0 in stage 28.0 (TID 28). 2527 bytes result sent to driver\n",
      "24/06/29 12:05:00 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 28) in 1397 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:05:00 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:05:00 INFO DAGScheduler: ResultStage 28 (save at NativeMethodAccessorImpl.java:0) finished in 1.412 s\n",
      "24/06/29 12:05:00 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:05:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished\n",
      "24/06/29 12:05:00 INFO DAGScheduler: Job 22 finished: save at NativeMethodAccessorImpl.java:0, took 1.414100 s\n",
      "\r",
      "                                                                                \r",
      "24/06/29 12:05:00 INFO FileFormatWriter: Start to commit write Job 8c656477-ac49-4a4b-b041-1da19c7ebe7e.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:00 INFO FileFormatWriter: Write Job 8c656477-ac49-4a4b-b041-1da19c7ebe7e committed. Elapsed time: 427 ms.\n",
      "24/06/29 12:05:00 INFO FileFormatWriter: Finished processing stats for write job 8c656477-ac49-4a4b-b041-1da19c7ebe7e.\n",
      "24/06/29 12:05:00 INFO CodeGenerator: Code generated in 3.380932 ms\n",
      "24/06/29 12:05:00 INFO SparkContext: Starting job: collect at /tmp/ipykernel_62125/3616262314.py:82\n",
      "24/06/29 12:05:00 INFO DAGScheduler: Got job 23 (collect at /tmp/ipykernel_62125/3616262314.py:82) with 1 output partitions\n",
      "24/06/29 12:05:00 INFO DAGScheduler: Final stage: ResultStage 29 (collect at /tmp/ipykernel_62125/3616262314.py:82)\n",
      "24/06/29 12:05:00 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:05:00 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:05:00 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[73] at collect at /tmp/ipykernel_62125/3616262314.py:82), which has no missing parents\n",
      "24/06/29 12:05:00 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 7.4 KiB, free 911.7 MiB)\n",
      "24/06/29 12:05:00 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 911.7 MiB)\n",
      "24/06/29 12:05:00 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 192.168.1.8:45173 (size: 3.8 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:05:00 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:05:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[73] at collect at /tmp/ipykernel_62125/3616262314.py:82) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:05:00 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:05:00 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 29) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7823 bytes) \n",
      "24/06/29 12:05:00 INFO Executor: Running task 0.0 in stage 29.0 (TID 29)\n",
      "24/06/29 12:05:00 INFO CodeGenerator: Code generated in 3.277575 ms\n",
      "24/06/29 12:05:00 INFO Executor: Finished task 0.0 in stage 29.0 (TID 29). 1419 bytes result sent to driver\n",
      "24/06/29 12:05:00 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 29) in 7 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:05:00 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:05:00 INFO DAGScheduler: ResultStage 29 (collect at /tmp/ipykernel_62125/3616262314.py:82) finished in 0.009 s\n",
      "24/06/29 12:05:00 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:05:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished\n",
      "24/06/29 12:05:00 INFO DAGScheduler: Job 23 finished: collect at /tmp/ipykernel_62125/3616262314.py:82, took 0.010787 s\n",
      "24/06/29 12:05:00 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:00 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:05:00 INFO DAGScheduler: Got job 24 (save at NativeMethodAccessorImpl.java:0) with 4 output partitions\n",
      "24/06/29 12:05:00 INFO DAGScheduler: Final stage: ResultStage 30 (save at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:05:00 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:05:00 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:05:00 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[80] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task  4   Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:00 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 220.5 KiB, free 911.5 MiB)\n",
      "24/06/29 12:05:00 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 78.7 KiB, free 911.4 MiB)\n",
      "24/06/29 12:05:00 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 192.168.1.8:45173 (size: 78.7 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:05:00 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:05:00 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 30 (MapPartitionsRDD[80] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "24/06/29 12:05:00 INFO TaskSchedulerImpl: Adding task set 30.0 with 4 tasks resource profile 0\n",
      "24/06/29 12:05:00 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 30) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/06/29 12:05:00 INFO TaskSetManager: Starting task 1.0 in stage 30.0 (TID 31) (192.168.1.8, executor driver, partition 1, PROCESS_LOCAL, 7595 bytes) \n",
      "24/06/29 12:05:00 INFO TaskSetManager: Starting task 2.0 in stage 30.0 (TID 32) (192.168.1.8, executor driver, partition 2, PROCESS_LOCAL, 7595 bytes) \n",
      "24/06/29 12:05:00 INFO TaskSetManager: Starting task 3.0 in stage 30.0 (TID 33) (192.168.1.8, executor driver, partition 3, PROCESS_LOCAL, 7809 bytes) \n",
      "24/06/29 12:05:00 INFO Executor: Running task 0.0 in stage 30.0 (TID 30)\n",
      "24/06/29 12:05:00 INFO Executor: Running task 2.0 in stage 30.0 (TID 32)\n",
      "24/06/29 12:05:00 INFO Executor: Running task 3.0 in stage 30.0 (TID 33)\n",
      "24/06/29 12:05:00 INFO Executor: Running task 1.0 in stage 30.0 (TID 31)\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:00 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:05:00 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:00 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/06/29 12:05:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"BatchId\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TaskName\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceConnection\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceDatabase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TargetTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"StartTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"EndTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceRowsRead\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetInserted\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetUpdated\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnMissing\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnNull\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Error\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Phase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional int32 BatchId;\n",
      "  optional binary TaskName (STRING);\n",
      "  optional binary SourceConnection (STRING);\n",
      "  optional binary SourceDatabase (STRING);\n",
      "  optional binary SourceTable (STRING);\n",
      "  optional binary TargetTable (STRING);\n",
      "  optional binary StartTime (STRING);\n",
      "  optional binary EndTime (STRING);\n",
      "  optional int32 SourceRowsRead;\n",
      "  optional int32 NumTargetInserted;\n",
      "  optional int32 NumTargetUpdated;\n",
      "  optional binary Status (STRING);\n",
      "  optional binary ColumnMissing (STRING);\n",
      "  optional binary ColumnNull (STRING);\n",
      "  optional binary Error (STRING);\n",
      "  optional binary Phase (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:00 INFO PythonRunner: Times: total = 68, boot = -44010, init = 44078, finish = 0\n",
      "24/06/29 12:05:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202406291205005206992519616159296_0030_m_000001_31\n",
      "24/06/29 12:05:00 INFO Executor: Finished task 1.0 in stage 30.0 (TID 31). 2789 bytes result sent to driver\n",
      "24/06/29 12:05:00 INFO PythonRunner: Times: total = 74, boot = -44013, init = 44087, finish = 0\n",
      "24/06/29 12:05:00 INFO TaskSetManager: Finished task 1.0 in stage 30.0 (TID 31) in 85 ms on 192.168.1.8 (executor driver) (1/4)\n",
      "24/06/29 12:05:00 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202406291205005206992519616159296_0030_m_000002_32\n",
      "24/06/29 12:05:00 INFO Executor: Finished task 2.0 in stage 30.0 (TID 32). 2789 bytes result sent to driver\n",
      "24/06/29 12:05:00 INFO TaskSetManager: Finished task 2.0 in stage 30.0 (TID 32) in 91 ms on 192.168.1.8 (executor driver) (2/4)\n",
      "24/06/29 12:05:00 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:05:00 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:05:00 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/06/29 12:05:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"BatchId\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TaskName\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceConnection\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceDatabase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TargetTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"StartTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"EndTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceRowsRead\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetInserted\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetUpdated\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnMissing\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnNull\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Error\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Phase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional int32 BatchId;\n",
      "  optional binary TaskName (STRING);\n",
      "  optional binary SourceConnection (STRING);\n",
      "  optional binary SourceDatabase (STRING);\n",
      "  optional binary SourceTable (STRING);\n",
      "  optional binary TargetTable (STRING);\n",
      "  optional binary StartTime (STRING);\n",
      "  optional binary EndTime (STRING);\n",
      "  optional int32 SourceRowsRead;\n",
      "  optional int32 NumTargetInserted;\n",
      "  optional int32 NumTargetUpdated;\n",
      "  optional binary Status (STRING);\n",
      "  optional binary ColumnMissing (STRING);\n",
      "  optional binary ColumnNull (STRING);\n",
      "  optional binary Error (STRING);\n",
      "  optional binary Phase (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/06/29 12:05:00 INFO PythonRunner: Times: total = 99, boot = -44010, init = 44109, finish = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:00 INFO PythonRunner: Times: total = 98, boot = -44013, init = 44111, finish = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 30:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:01 INFO FileOutputCommitter: Saved output of task 'attempt_202406291205005206992519616159296_0030_m_000000_30' to hdfs://localhost:9000/lakehouse/LH_Global_Electronics_Retailer/Files/log/2024-06-29/batch_8/_temporary/0/task_202406291205005206992519616159296_0030_m_000000\n",
      "24/06/29 12:05:01 INFO SparkHadoopMapRedUtil: attempt_202406291205005206992519616159296_0030_m_000000_30: Committed. Elapsed time: 357 ms.\n",
      "24/06/29 12:05:01 INFO Executor: Finished task 0.0 in stage 30.0 (TID 30). 2832 bytes result sent to driver\n",
      "24/06/29 12:05:01 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 30) in 1055 ms on 192.168.1.8 (executor driver) (3/4)\n",
      "24/06/29 12:05:01 INFO FileOutputCommitter: Saved output of task 'attempt_202406291205005206992519616159296_0030_m_000003_33' to hdfs://localhost:9000/lakehouse/LH_Global_Electronics_Retailer/Files/log/2024-06-29/batch_8/_temporary/0/task_202406291205005206992519616159296_0030_m_000003\n",
      "24/06/29 12:05:01 INFO SparkHadoopMapRedUtil: attempt_202406291205005206992519616159296_0030_m_000003_33: Committed. Elapsed time: 284 ms.\n",
      "24/06/29 12:05:01 INFO Executor: Finished task 3.0 in stage 30.0 (TID 33). 2875 bytes result sent to driver\n",
      "24/06/29 12:05:01 INFO TaskSetManager: Finished task 3.0 in stage 30.0 (TID 33) in 1147 ms on 192.168.1.8 (executor driver) (4/4)\n",
      "24/06/29 12:05:01 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:05:01 INFO DAGScheduler: ResultStage 30 (save at NativeMethodAccessorImpl.java:0) finished in 1.170 s\n",
      "24/06/29 12:05:01 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:05:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished\n",
      "24/06/29 12:05:01 INFO DAGScheduler: Job 24 finished: save at NativeMethodAccessorImpl.java:0, took 1.174205 s\n",
      "\r",
      "                                                                                \r",
      "24/06/29 12:05:01 INFO FileFormatWriter: Start to commit write Job 7c61aabd-9e89-4822-aeeb-cf382c5684f5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:02 INFO FileFormatWriter: Write Job 7c61aabd-9e89-4822-aeeb-cf382c5684f5 committed. Elapsed time: 469 ms.\n",
      "24/06/29 12:05:02 INFO FileFormatWriter: Finished processing stats for write job 7c61aabd-9e89-4822-aeeb-cf382c5684f5.\n",
      "24/06/29 12:05:02 INFO CodeGenerator: Code generated in 3.221458 ms\n",
      "24/06/29 12:05:02 INFO SparkContext: Starting job: collect at /tmp/ipykernel_62125/3616262314.py:43\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Got job 25 (collect at /tmp/ipykernel_62125/3616262314.py:43) with 1 output partitions\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Final stage: ResultStage 31 (collect at /tmp/ipykernel_62125/3616262314.py:43)\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[82] at collect at /tmp/ipykernel_62125/3616262314.py:43), which has no missing parents\n",
      "24/06/29 12:05:02 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 7.4 KiB, free 911.4 MiB)\n",
      "24/06/29 12:05:02 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 911.4 MiB)\n",
      "24/06/29 12:05:02 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 192.168.1.8:45173 (size: 3.8 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:05:02 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[82] at collect at /tmp/ipykernel_62125/3616262314.py:43) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:05:02 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:05:02 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 34) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7823 bytes) \n",
      "24/06/29 12:05:02 INFO Executor: Running task 0.0 in stage 31.0 (TID 34)\n",
      "24/06/29 12:05:02 INFO CodeGenerator: Code generated in 4.248593 ms\n",
      "24/06/29 12:05:02 INFO Executor: Finished task 0.0 in stage 31.0 (TID 34). 1419 bytes result sent to driver\n",
      "24/06/29 12:05:02 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 34) in 8 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:05:02 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:05:02 INFO DAGScheduler: ResultStage 31 (collect at /tmp/ipykernel_62125/3616262314.py:43) finished in 0.010 s\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:05:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Job 25 finished: collect at /tmp/ipykernel_62125/3616262314.py:43, took 0.011563 s\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Registering RDD 85 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 6\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Got map stage job 26 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Final stage: ShuffleMapStage 32 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[85] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:05:02 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 13.8 KiB, free 911.4 MiB)\n",
      "24/06/29 12:05:02 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 911.4 MiB)\n",
      "24/06/29 12:05:02 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 192.168.1.8:45173 (size: 7.2 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:05:02 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[85] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:05:02 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:05:02 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 35) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7450 bytes) \n",
      "24/06/29 12:05:02 INFO Executor: Running task 0.0 in stage 32.0 (TID 35)\n",
      "24/06/29 12:05:02 INFO JDBCRDD: closed connection\n",
      "24/06/29 12:05:02 INFO Executor: Finished task 0.0 in stage 32.0 (TID 35). 1884 bytes result sent to driver\n",
      "24/06/29 12:05:02 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 35) in 11 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:05:02 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:05:02 INFO DAGScheduler: ShuffleMapStage 32 (count at NativeMethodAccessorImpl.java:0) finished in 0.014 s\n",
      "24/06/29 12:05:02 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/06/29 12:05:02 INFO DAGScheduler: running: Set()\n",
      "24/06/29 12:05:02 INFO DAGScheduler: waiting: Set()\n",
      "24/06/29 12:05:02 INFO DAGScheduler: failed: Set()\n",
      "24/06/29 12:05:02 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Got job 27 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Final stage: ResultStage 34 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[88] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:05:02 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 12.5 KiB, free 911.4 MiB)\n",
      "24/06/29 12:05:02 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 911.4 MiB)\n",
      "24/06/29 12:05:02 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 192.168.1.8:45173 (size: 5.9 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:05:02 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[88] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:05:02 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:05:02 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 36) (192.168.1.8, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/06/29 12:05:02 INFO Executor: Running task 0.0 in stage 34.0 (TID 36)\n",
      "24/06/29 12:05:02 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/06/29 12:05:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/06/29 12:05:02 INFO Executor: Finished task 0.0 in stage 34.0 (TID 36). 3952 bytes result sent to driver\n",
      "24/06/29 12:05:02 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 36) in 3 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:05:02 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:05:02 INFO DAGScheduler: ResultStage 34 (count at NativeMethodAccessorImpl.java:0) finished in 0.006 s\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:05:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Job 27 finished: count at NativeMethodAccessorImpl.java:0, took 0.007456 s\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Registering RDD 91 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 7\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Got map stage job 28 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Final stage: ShuffleMapStage 35 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Submitting ShuffleMapStage 35 (MapPartitionsRDD[91] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:05:02 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 13.8 KiB, free 911.3 MiB)\n",
      "24/06/29 12:05:02 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 911.3 MiB)\n",
      "24/06/29 12:05:02 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 192.168.1.8:45173 (size: 7.2 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:05:02 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[91] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:05:02 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:05:02 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 37) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7450 bytes) \n",
      "24/06/29 12:05:02 INFO Executor: Running task 0.0 in stage 35.0 (TID 37)\n",
      "24/06/29 12:05:02 INFO JDBCRDD: closed connection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:02 INFO Executor: Finished task 0.0 in stage 35.0 (TID 37). 1884 bytes result sent to driver\n",
      "24/06/29 12:05:02 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 37) in 101 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:05:02 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:05:02 INFO DAGScheduler: ShuffleMapStage 35 (count at NativeMethodAccessorImpl.java:0) finished in 0.104 s\n",
      "24/06/29 12:05:02 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/06/29 12:05:02 INFO DAGScheduler: running: Set()\n",
      "24/06/29 12:05:02 INFO DAGScheduler: waiting: Set()\n",
      "24/06/29 12:05:02 INFO DAGScheduler: failed: Set()\n",
      "24/06/29 12:05:02 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Got job 29 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Final stage: ResultStage 37 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[94] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:05:02 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 12.5 KiB, free 911.3 MiB)\n",
      "24/06/29 12:05:02 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 911.3 MiB)\n",
      "24/06/29 12:05:02 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 192.168.1.8:45173 (size: 5.9 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:05:02 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[94] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:05:02 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:05:02 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 38) (192.168.1.8, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/06/29 12:05:02 INFO Executor: Running task 0.0 in stage 37.0 (TID 38)\n",
      "24/06/29 12:05:02 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/06/29 12:05:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/06/29 12:05:02 INFO Executor: Finished task 0.0 in stage 37.0 (TID 38). 3995 bytes result sent to driver\n",
      "24/06/29 12:05:02 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 38) in 4 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:05:02 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:05:02 INFO DAGScheduler: ResultStage 37 (count at NativeMethodAccessorImpl.java:0) finished in 0.006 s\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:05:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished\n",
      "24/06/29 12:05:02 INFO DAGScheduler: Job 29 finished: count at NativeMethodAccessorImpl.java:0, took 0.008031 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:43 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:43 INFO CodeGenerator: Code generated in 12.717004 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:44 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:05:44 INFO DAGScheduler: Got job 30 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:05:44 INFO DAGScheduler: Final stage: ResultStage 38 (save at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:05:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:05:44 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:05:44 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[97] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:05:44 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 215.8 KiB, free 911.1 MiB)\n",
      "24/06/29 12:05:44 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 77.4 KiB, free 911.0 MiB)\n",
      "24/06/29 12:05:44 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 192.168.1.8:45173 (size: 77.4 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:05:44 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:05:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[97] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:05:44 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:05:44 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 39) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7461 bytes) \n",
      "24/06/29 12:05:44 INFO Executor: Running task 0.0 in stage 38.0 (TID 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:44 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 192.168.1.8:45173 in memory (size: 5.9 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:05:44 INFO CodeGenerator: Code generated in 13.246896 ms\n",
      "24/06/29 12:05:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:44 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:05:44 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:05:44 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/06/29 12:05:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"StoreKey\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Country\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"State\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Square Meters\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Open Date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"year\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"month\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"day\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional int32 StoreKey;\n",
      "  optional binary Country (STRING);\n",
      "  optional binary State (STRING);\n",
      "  optional int32 Square Meters;\n",
      "  optional binary Open Date (STRING);\n",
      "  required binary year (STRING);\n",
      "  required binary month (STRING);\n",
      "  required binary day (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/06/29 12:05:44 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 192.168.1.8:45173 in memory (size: 3.8 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:05:44 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 192.168.1.8:45173 in memory (size: 78.7 KiB, free: 912.0 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:44 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 192.168.1.8:45173 in memory (size: 3.8 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:05:44 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 192.168.1.8:45173 in memory (size: 7.2 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:05:44 INFO JDBCRDD: closed connection\n",
      "24/06/29 12:05:44 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 192.168.1.8:45173 in memory (size: 5.9 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:05:44 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 192.168.1.8:45173 in memory (size: 7.2 KiB, free: 912.1 MiB)\n",
      "\r",
      "[Stage 38:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:45 INFO FileOutputCommitter: Saved output of task 'attempt_202406291205444302968515625766198_0038_m_000000_39' to hdfs://localhost:9000/lakehouse/LH_Global_Electronics_Retailer/Files/Bronze/stores/year=2024/month=06/day=29/stores_2024_06_29-version_8.parquet/_temporary/0/task_202406291205444302968515625766198_0038_m_000000\n",
      "24/06/29 12:05:45 INFO SparkHadoopMapRedUtil: attempt_202406291205444302968515625766198_0038_m_000000_39: Committed. Elapsed time: 156 ms.\n",
      "24/06/29 12:05:45 INFO Executor: Finished task 0.0 in stage 38.0 (TID 39). 2484 bytes result sent to driver\n",
      "24/06/29 12:05:45 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 39) in 1265 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:05:45 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:05:45 INFO DAGScheduler: ResultStage 38 (save at NativeMethodAccessorImpl.java:0) finished in 1.421 s\n",
      "24/06/29 12:05:45 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:05:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished\n",
      "24/06/29 12:05:45 INFO DAGScheduler: Job 30 finished: save at NativeMethodAccessorImpl.java:0, took 1.424340 s\n",
      "\r",
      "                                                                                \r",
      "24/06/29 12:05:45 INFO FileFormatWriter: Start to commit write Job 64f87fb5-a07d-4df4-9a19-aba1ad86e4bd.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:45 INFO FileFormatWriter: Write Job 64f87fb5-a07d-4df4-9a19-aba1ad86e4bd committed. Elapsed time: 328 ms.\n",
      "24/06/29 12:05:45 INFO FileFormatWriter: Finished processing stats for write job 64f87fb5-a07d-4df4-9a19-aba1ad86e4bd.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task  2   Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:46 INFO CodeGenerator: Code generated in 4.000988 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:46 INFO SparkContext: Starting job: collect at /tmp/ipykernel_62125/3616262314.py:82\n",
      "24/06/29 12:05:46 INFO DAGScheduler: Got job 31 (collect at /tmp/ipykernel_62125/3616262314.py:82) with 1 output partitions\n",
      "24/06/29 12:05:46 INFO DAGScheduler: Final stage: ResultStage 39 (collect at /tmp/ipykernel_62125/3616262314.py:82)\n",
      "24/06/29 12:05:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:05:46 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:05:46 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[99] at collect at /tmp/ipykernel_62125/3616262314.py:82), which has no missing parents\n",
      "24/06/29 12:05:46 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 7.4 KiB, free 911.4 MiB)\n",
      "24/06/29 12:05:46 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 911.4 MiB)\n",
      "24/06/29 12:05:46 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 192.168.1.8:45173 (size: 3.8 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:05:46 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:05:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[99] at collect at /tmp/ipykernel_62125/3616262314.py:82) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:05:46 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:05:46 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 40) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7823 bytes) \n",
      "24/06/29 12:05:46 INFO Executor: Running task 0.0 in stage 39.0 (TID 40)\n",
      "24/06/29 12:05:46 INFO CodeGenerator: Code generated in 9.381807 ms\n",
      "24/06/29 12:05:46 INFO Executor: Finished task 0.0 in stage 39.0 (TID 40). 1462 bytes result sent to driver\n",
      "24/06/29 12:05:46 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 40) in 18 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:05:46 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:05:46 INFO DAGScheduler: ResultStage 39 (collect at /tmp/ipykernel_62125/3616262314.py:82) finished in 0.064 s\n",
      "24/06/29 12:05:46 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:05:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished\n",
      "24/06/29 12:05:46 INFO DAGScheduler: Job 31 finished: collect at /tmp/ipykernel_62125/3616262314.py:82, took 0.067330 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:47 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:47 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:05:47 INFO DAGScheduler: Got job 32 (save at NativeMethodAccessorImpl.java:0) with 4 output partitions\n",
      "24/06/29 12:05:47 INFO DAGScheduler: Final stage: ResultStage 40 (save at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:05:47 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:05:47 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:05:47 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[106] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:05:47 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 220.5 KiB, free 911.2 MiB)\n",
      "24/06/29 12:05:47 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 78.7 KiB, free 911.1 MiB)\n",
      "24/06/29 12:05:47 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 192.168.1.8:45173 (size: 78.7 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:05:47 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:05:47 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 40 (MapPartitionsRDD[106] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "24/06/29 12:05:47 INFO TaskSchedulerImpl: Adding task set 40.0 with 4 tasks resource profile 0\n",
      "24/06/29 12:05:47 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 41) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/06/29 12:05:47 INFO TaskSetManager: Starting task 1.0 in stage 40.0 (TID 42) (192.168.1.8, executor driver, partition 1, PROCESS_LOCAL, 7595 bytes) \n",
      "24/06/29 12:05:47 INFO TaskSetManager: Starting task 2.0 in stage 40.0 (TID 43) (192.168.1.8, executor driver, partition 2, PROCESS_LOCAL, 7595 bytes) \n",
      "24/06/29 12:05:47 INFO TaskSetManager: Starting task 3.0 in stage 40.0 (TID 44) (192.168.1.8, executor driver, partition 3, PROCESS_LOCAL, 7809 bytes) \n",
      "24/06/29 12:05:47 INFO Executor: Running task 0.0 in stage 40.0 (TID 41)\n",
      "24/06/29 12:05:47 INFO Executor: Running task 1.0 in stage 40.0 (TID 42)\n",
      "24/06/29 12:05:47 INFO Executor: Running task 2.0 in stage 40.0 (TID 43)\n",
      "24/06/29 12:05:47 INFO Executor: Running task 3.0 in stage 40.0 (TID 44)\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:47 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:05:47 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:47 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/06/29 12:05:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:05:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:05:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:05:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"BatchId\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TaskName\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceConnection\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceDatabase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TargetTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"StartTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"EndTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceRowsRead\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetInserted\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetUpdated\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnMissing\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnNull\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Error\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Phase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional int32 BatchId;\n",
      "  optional binary TaskName (STRING);\n",
      "  optional binary SourceConnection (STRING);\n",
      "  optional binary SourceDatabase (STRING);\n",
      "  optional binary SourceTable (STRING);\n",
      "  optional binary TargetTable (STRING);\n",
      "  optional binary StartTime (STRING);\n",
      "  optional binary EndTime (STRING);\n",
      "  optional int32 SourceRowsRead;\n",
      "  optional int32 NumTargetInserted;\n",
      "  optional int32 NumTargetUpdated;\n",
      "  optional binary Status (STRING);\n",
      "  optional binary ColumnMissing (STRING);\n",
      "  optional binary ColumnNull (STRING);\n",
      "  optional binary Error (STRING);\n",
      "  optional binary Phase (STRING);\n",
      "}\n",
      "\n",
      "       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 40:>                                                         (0 + 4) / 4]\r",
      "24/06/29 12:05:48 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:05:48 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:05:48 INFO PythonRunner: Times: total = 644, boot = -46590, init = 47234, finish = 0\n",
      "24/06/29 12:05:48 INFO PythonRunner: Times: total = 647, boot = -46588, init = 47235, finish = 0\n",
      "24/06/29 12:05:48 INFO PythonRunner: Times: total = 646, boot = -46568, init = 47214, finish = 0\n",
      "24/06/29 12:05:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202406291205477603984521574155561_0040_m_000001_42\n",
      "24/06/29 12:05:48 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/06/29 12:05:48 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"BatchId\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TaskName\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceConnection\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceDatabase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TargetTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"StartTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"EndTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceRowsRead\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetInserted\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetUpdated\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnMissing\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnNull\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Error\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Phase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional int32 BatchId;\n",
      "  optional binary TaskName (STRING);\n",
      "  optional binary SourceConnection (STRING);\n",
      "  optional binary SourceDatabase (STRING);\n",
      "  optional binary SourceTable (STRING);\n",
      "  optional binary TargetTable (STRING);\n",
      "  optional binary StartTime (STRING);\n",
      "  optional binary EndTime (STRING);\n",
      "  optional int32 SourceRowsRead;\n",
      "  optional int32 NumTargetInserted;\n",
      "  optional int32 NumTargetUpdated;\n",
      "  optional binary Status (STRING);\n",
      "  optional binary ColumnMissing (STRING);\n",
      "  optional binary ColumnNull (STRING);\n",
      "  optional binary Error (STRING);\n",
      "  optional binary Phase (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/06/29 12:05:48 INFO Executor: Finished task 1.0 in stage 40.0 (TID 42). 2789 bytes result sent to driver\n",
      "24/06/29 12:05:48 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202406291205477603984521574155561_0040_m_000002_43\n",
      "24/06/29 12:05:48 INFO TaskSetManager: Finished task 1.0 in stage 40.0 (TID 42) in 670 ms on 192.168.1.8 (executor driver) (1/4)\n",
      "24/06/29 12:05:48 INFO Executor: Finished task 2.0 in stage 40.0 (TID 43). 2789 bytes result sent to driver\n",
      "24/06/29 12:05:48 INFO TaskSetManager: Finished task 2.0 in stage 40.0 (TID 43) in 677 ms on 192.168.1.8 (executor driver) (2/4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 40:=============================>                            (2 + 2) / 4]\r",
      "24/06/29 12:05:48 INFO PythonRunner: Times: total = 643, boot = -46568, init = 47211, finish = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:48 INFO FileOutputCommitter: Saved output of task 'attempt_202406291205477603984521574155561_0040_m_000000_41' to hdfs://localhost:9000/lakehouse/LH_Global_Electronics_Retailer/Files/log/2024-06-29/batch_8/_temporary/0/task_202406291205477603984521574155561_0040_m_000000\n",
      "24/06/29 12:05:48 INFO SparkHadoopMapRedUtil: attempt_202406291205477603984521574155561_0040_m_000000_41: Committed. Elapsed time: 94 ms.\n",
      "24/06/29 12:05:48 INFO Executor: Finished task 0.0 in stage 40.0 (TID 41). 2832 bytes result sent to driver\n",
      "24/06/29 12:05:48 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 41) in 991 ms on 192.168.1.8 (executor driver) (3/4)\n",
      "24/06/29 12:05:48 INFO FileOutputCommitter: Saved output of task 'attempt_202406291205477603984521574155561_0040_m_000003_44' to hdfs://localhost:9000/lakehouse/LH_Global_Electronics_Retailer/Files/log/2024-06-29/batch_8/_temporary/0/task_202406291205477603984521574155561_0040_m_000003\n",
      "24/06/29 12:05:48 INFO SparkHadoopMapRedUtil: attempt_202406291205477603984521574155561_0040_m_000003_44: Committed. Elapsed time: 98 ms.\n",
      "24/06/29 12:05:48 INFO Executor: Finished task 3.0 in stage 40.0 (TID 44). 2875 bytes result sent to driver\n",
      "24/06/29 12:05:48 INFO TaskSetManager: Finished task 3.0 in stage 40.0 (TID 44) in 1112 ms on 192.168.1.8 (executor driver) (4/4)\n",
      "24/06/29 12:05:48 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:05:48 INFO DAGScheduler: ResultStage 40 (save at NativeMethodAccessorImpl.java:0) finished in 1.159 s\n",
      "24/06/29 12:05:48 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:05:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished\n",
      "24/06/29 12:05:48 INFO DAGScheduler: Job 32 finished: save at NativeMethodAccessorImpl.java:0, took 1.162554 s\n",
      "\r",
      "                                                                                \r",
      "24/06/29 12:05:48 INFO FileFormatWriter: Start to commit write Job c7f7208a-6883-4e6a-9807-c3cca9089951.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:48 INFO FileFormatWriter: Write Job c7f7208a-6883-4e6a-9807-c3cca9089951 committed. Elapsed time: 284 ms.\n",
      "24/06/29 12:05:48 INFO FileFormatWriter: Finished processing stats for write job c7f7208a-6883-4e6a-9807-c3cca9089951.\n",
      "24/06/29 12:05:48 INFO CodeGenerator: Code generated in 5.070575 ms\n",
      "24/06/29 12:05:48 INFO SparkContext: Starting job: collect at /tmp/ipykernel_62125/3616262314.py:43\n",
      "24/06/29 12:05:48 INFO DAGScheduler: Got job 33 (collect at /tmp/ipykernel_62125/3616262314.py:43) with 1 output partitions\n",
      "24/06/29 12:05:48 INFO DAGScheduler: Final stage: ResultStage 41 (collect at /tmp/ipykernel_62125/3616262314.py:43)\n",
      "24/06/29 12:05:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:05:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:05:48 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[108] at collect at /tmp/ipykernel_62125/3616262314.py:43), which has no missing parents\n",
      "24/06/29 12:05:48 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 7.4 KiB, free 911.1 MiB)\n",
      "24/06/29 12:05:48 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 911.1 MiB)\n",
      "24/06/29 12:05:48 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 192.168.1.8:45173 (size: 3.8 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:05:48 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:05:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[108] at collect at /tmp/ipykernel_62125/3616262314.py:43) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:05:48 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:05:48 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 45) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7823 bytes) \n",
      "24/06/29 12:05:48 INFO Executor: Running task 0.0 in stage 41.0 (TID 45)\n",
      "24/06/29 12:05:48 INFO CodeGenerator: Code generated in 9.785405 ms\n",
      "24/06/29 12:05:48 INFO Executor: Finished task 0.0 in stage 41.0 (TID 45). 1462 bytes result sent to driver\n",
      "24/06/29 12:05:48 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 45) in 17 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:05:48 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:05:48 INFO DAGScheduler: ResultStage 41 (collect at /tmp/ipykernel_62125/3616262314.py:43) finished in 0.076 s\n",
      "24/06/29 12:05:48 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:05:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished\n",
      "24/06/29 12:05:48 INFO DAGScheduler: Job 33 finished: collect at /tmp/ipykernel_62125/3616262314.py:43, took 0.077591 s\n",
      "24/06/29 12:05:48 INFO DAGScheduler: Registering RDD 111 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 8\n",
      "24/06/29 12:05:48 INFO DAGScheduler: Got map stage job 34 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:05:48 INFO DAGScheduler: Final stage: ShuffleMapStage 42 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:05:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:05:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:05:48 INFO DAGScheduler: Submitting ShuffleMapStage 42 (MapPartitionsRDD[111] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:05:48 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 13.8 KiB, free 911.1 MiB)\n",
      "24/06/29 12:05:48 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 911.1 MiB)\n",
      "24/06/29 12:05:48 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 192.168.1.8:45173 (size: 7.2 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:05:48 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:05:48 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 42 (MapPartitionsRDD[111] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:05:48 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:05:48 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 46) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7450 bytes) \n",
      "24/06/29 12:05:48 INFO Executor: Running task 0.0 in stage 42.0 (TID 46)\n",
      "24/06/29 12:05:49 INFO JDBCRDD: closed connection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:05:49 INFO Executor: Finished task 0.0 in stage 42.0 (TID 46). 1884 bytes result sent to driver\n",
      "24/06/29 12:05:49 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 46) in 129 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:05:49 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:05:49 INFO DAGScheduler: ShuffleMapStage 42 (count at NativeMethodAccessorImpl.java:0) finished in 0.159 s\n",
      "24/06/29 12:05:49 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/06/29 12:05:49 INFO DAGScheduler: running: Set()\n",
      "24/06/29 12:05:49 INFO DAGScheduler: waiting: Set()\n",
      "24/06/29 12:05:49 INFO DAGScheduler: failed: Set()\n",
      "24/06/29 12:05:49 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Got job 35 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Final stage: ResultStage 44 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 43)\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[114] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:05:49 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 12.5 KiB, free 911.1 MiB)\n",
      "24/06/29 12:05:49 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 911.1 MiB)\n",
      "24/06/29 12:05:49 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 192.168.1.8:45173 (size: 5.9 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:05:49 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[114] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:05:49 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:05:49 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 47) (192.168.1.8, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/06/29 12:05:49 INFO Executor: Running task 0.0 in stage 44.0 (TID 47)\n",
      "24/06/29 12:05:49 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/06/29 12:05:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/06/29 12:05:49 INFO Executor: Finished task 0.0 in stage 44.0 (TID 47). 3995 bytes result sent to driver\n",
      "24/06/29 12:05:49 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 47) in 6 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:05:49 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:05:49 INFO DAGScheduler: ResultStage 44 (count at NativeMethodAccessorImpl.java:0) finished in 0.017 s\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:05:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 44: Stage finished\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Job 35 finished: count at NativeMethodAccessorImpl.java:0, took 0.019081 s\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Registering RDD 117 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 9\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Got map stage job 36 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Final stage: ShuffleMapStage 45 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Submitting ShuffleMapStage 45 (MapPartitionsRDD[117] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:05:49 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 13.8 KiB, free 911.1 MiB)\n",
      "24/06/29 12:05:49 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 911.0 MiB)\n",
      "24/06/29 12:05:49 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 192.168.1.8:45173 (size: 7.2 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:05:49 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[117] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:05:49 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:05:49 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 48) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7450 bytes) \n",
      "24/06/29 12:05:49 INFO Executor: Running task 0.0 in stage 45.0 (TID 48)\n",
      "24/06/29 12:05:49 INFO JDBCRDD: closed connection\n",
      "24/06/29 12:05:49 INFO Executor: Finished task 0.0 in stage 45.0 (TID 48). 1884 bytes result sent to driver\n",
      "24/06/29 12:05:49 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 48) in 46 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:05:49 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:05:49 INFO DAGScheduler: ShuffleMapStage 45 (count at NativeMethodAccessorImpl.java:0) finished in 0.092 s\n",
      "24/06/29 12:05:49 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/06/29 12:05:49 INFO DAGScheduler: running: Set()\n",
      "24/06/29 12:05:49 INFO DAGScheduler: waiting: Set()\n",
      "24/06/29 12:05:49 INFO DAGScheduler: failed: Set()\n",
      "24/06/29 12:05:49 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Got job 37 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Final stage: ResultStage 47 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 46)\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[120] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:05:49 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 12.5 KiB, free 911.0 MiB)\n",
      "24/06/29 12:05:49 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 911.0 MiB)\n",
      "24/06/29 12:05:49 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 192.168.1.8:45173 (size: 5.9 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:05:49 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (MapPartitionsRDD[120] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:05:49 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:05:49 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 49) (192.168.1.8, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/06/29 12:05:49 INFO Executor: Running task 0.0 in stage 47.0 (TID 49)\n",
      "24/06/29 12:05:49 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/06/29 12:05:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/06/29 12:05:49 INFO Executor: Finished task 0.0 in stage 47.0 (TID 49). 3995 bytes result sent to driver\n",
      "24/06/29 12:05:49 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 49) in 3 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:05:49 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:05:49 INFO DAGScheduler: ResultStage 47 (count at NativeMethodAccessorImpl.java:0) finished in 0.006 s\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:05:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished\n",
      "24/06/29 12:05:49 INFO DAGScheduler: Job 37 finished: count at NativeMethodAccessorImpl.java:0, took 0.007987 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:06:30 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:06:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:06:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:06:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:06:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:06:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:06:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:06:30 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:06:30 INFO DAGScheduler: Got job 38 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/29 12:06:30 INFO DAGScheduler: Final stage: ResultStage 48 (save at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:06:30 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:06:30 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:06:30 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[123] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:06:30 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 218.1 KiB, free 910.8 MiB)\n",
      "24/06/29 12:06:31 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 77.8 KiB, free 910.7 MiB)\n",
      "24/06/29 12:06:31 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 192.168.1.8:45173 (size: 77.8 KiB, free: 911.9 MiB)\n",
      "24/06/29 12:06:31 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:06:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[123] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:06:31 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:06:31 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 50) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7461 bytes) \n",
      "24/06/29 12:06:31 INFO Executor: Running task 0.0 in stage 48.0 (TID 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:06:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:06:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:06:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:06:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:06:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:06:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:06:31 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:06:31 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:06:31 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/06/29 12:06:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"CustomerKey\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Gender\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"City\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"State Code\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"State\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Zip Code\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Country\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Continent\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"Birthday\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : {\n",
      "      \"scale\" : 0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"year\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"month\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"day\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional int32 CustomerKey;\n",
      "  optional binary Gender (STRING);\n",
      "  optional binary Name (STRING);\n",
      "  optional binary City (STRING);\n",
      "  optional binary State Code (STRING);\n",
      "  optional binary State (STRING);\n",
      "  optional binary Zip Code (STRING);\n",
      "  optional binary Country (STRING);\n",
      "  optional binary Continent (STRING);\n",
      "  optional binary Birthday (STRING);\n",
      "  required binary year (STRING);\n",
      "  required binary month (STRING);\n",
      "  required binary day (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/06/29 12:06:31 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 192.168.1.8:45173 in memory (size: 5.9 KiB, free: 911.9 MiB)\n",
      "24/06/29 12:06:31 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 192.168.1.8:45173 in memory (size: 3.8 KiB, free: 911.9 MiB)\n",
      "24/06/29 12:06:31 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 192.168.1.8:45173 in memory (size: 78.7 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:06:31 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 192.168.1.8:45173 in memory (size: 5.9 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:06:31 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 192.168.1.8:45173 in memory (size: 7.2 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:06:31 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 192.168.1.8:45173 in memory (size: 7.2 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:06:31 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 192.168.1.8:45173 in memory (size: 77.4 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:06:31 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 192.168.1.8:45173 in memory (size: 3.8 KiB, free: 912.1 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:06:31 INFO JDBCRDD: closed connection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:06:31 INFO FileOutputCommitter: Saved output of task 'attempt_202406291206304705055975322099111_0048_m_000000_50' to hdfs://localhost:9000/lakehouse/LH_Global_Electronics_Retailer/Files/Bronze/customers/year=2024/month=06/day=29/customers_2024_06_29-version_8.parquet/_temporary/0/task_202406291206304705055975322099111_0048_m_000000\n",
      "24/06/29 12:06:31 INFO SparkHadoopMapRedUtil: attempt_202406291206304705055975322099111_0048_m_000000_50: Committed. Elapsed time: 49 ms.\n",
      "24/06/29 12:06:31 INFO Executor: Finished task 0.0 in stage 48.0 (TID 50). 2527 bytes result sent to driver\n",
      "24/06/29 12:06:31 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 50) in 512 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:06:31 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:06:31 INFO DAGScheduler: ResultStage 48 (save at NativeMethodAccessorImpl.java:0) finished in 0.578 s\n",
      "24/06/29 12:06:31 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:06:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished\n",
      "24/06/29 12:06:31 INFO DAGScheduler: Job 38 finished: save at NativeMethodAccessorImpl.java:0, took 0.579676 s\n",
      "24/06/29 12:06:31 INFO FileFormatWriter: Start to commit write Job a01189fb-d0f7-4fda-bd45-6be913673ed1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:06:32 INFO FileFormatWriter: Write Job a01189fb-d0f7-4fda-bd45-6be913673ed1 committed. Elapsed time: 704 ms.\n",
      "24/06/29 12:06:32 INFO FileFormatWriter: Finished processing stats for write job a01189fb-d0f7-4fda-bd45-6be913673ed1.\n",
      "24/06/29 12:06:32 INFO CodeGenerator: Code generated in 4.557123 ms\n",
      "24/06/29 12:06:32 INFO SparkContext: Starting job: collect at /tmp/ipykernel_62125/3616262314.py:82\n",
      "24/06/29 12:06:32 INFO DAGScheduler: Got job 39 (collect at /tmp/ipykernel_62125/3616262314.py:82) with 1 output partitions\n",
      "24/06/29 12:06:32 INFO DAGScheduler: Final stage: ResultStage 49 (collect at /tmp/ipykernel_62125/3616262314.py:82)\n",
      "24/06/29 12:06:32 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:06:32 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:06:32 INFO DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[125] at collect at /tmp/ipykernel_62125/3616262314.py:82), which has no missing parents\n",
      "24/06/29 12:06:32 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 7.4 KiB, free 911.4 MiB)\n",
      "24/06/29 12:06:32 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 911.4 MiB)\n",
      "24/06/29 12:06:32 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 192.168.1.8:45173 (size: 3.8 KiB, free: 912.1 MiB)\n",
      "24/06/29 12:06:32 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:06:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[125] at collect at /tmp/ipykernel_62125/3616262314.py:82) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/29 12:06:32 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks resource profile 0\n",
      "24/06/29 12:06:32 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 51) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7823 bytes) \n",
      "24/06/29 12:06:32 INFO Executor: Running task 0.0 in stage 49.0 (TID 51)\n",
      "24/06/29 12:06:32 INFO CodeGenerator: Code generated in 3.058926 ms\n",
      "24/06/29 12:06:32 INFO Executor: Finished task 0.0 in stage 49.0 (TID 51). 1419 bytes result sent to driver\n",
      "24/06/29 12:06:32 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 51) in 66 ms on 192.168.1.8 (executor driver) (1/1)\n",
      "24/06/29 12:06:32 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:06:32 INFO DAGScheduler: ResultStage 49 (collect at /tmp/ipykernel_62125/3616262314.py:82) finished in 0.068 s\n",
      "24/06/29 12:06:32 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:06:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 49: Stage finished\n",
      "24/06/29 12:06:32 INFO DAGScheduler: Job 39 finished: collect at /tmp/ipykernel_62125/3616262314.py:82, took 0.069835 s\n",
      "24/06/29 12:06:32 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:06:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:06:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task  1   Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:06:32 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "24/06/29 12:06:32 INFO DAGScheduler: Got job 40 (save at NativeMethodAccessorImpl.java:0) with 4 output partitions\n",
      "24/06/29 12:06:32 INFO DAGScheduler: Final stage: ResultStage 50 (save at NativeMethodAccessorImpl.java:0)\n",
      "24/06/29 12:06:32 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/29 12:06:32 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/29 12:06:32 INFO DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[132] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/29 12:06:32 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 220.5 KiB, free 911.2 MiB)\n",
      "24/06/29 12:06:32 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 78.7 KiB, free 911.1 MiB)\n",
      "24/06/29 12:06:32 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 192.168.1.8:45173 (size: 78.7 KiB, free: 912.0 MiB)\n",
      "24/06/29 12:06:32 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/29 12:06:32 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 50 (MapPartitionsRDD[132] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "24/06/29 12:06:32 INFO TaskSchedulerImpl: Adding task set 50.0 with 4 tasks resource profile 0\n",
      "24/06/29 12:06:32 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 52) (192.168.1.8, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/06/29 12:06:32 INFO TaskSetManager: Starting task 1.0 in stage 50.0 (TID 53) (192.168.1.8, executor driver, partition 1, PROCESS_LOCAL, 7595 bytes) \n",
      "24/06/29 12:06:32 INFO TaskSetManager: Starting task 2.0 in stage 50.0 (TID 54) (192.168.1.8, executor driver, partition 2, PROCESS_LOCAL, 7595 bytes) \n",
      "24/06/29 12:06:32 INFO TaskSetManager: Starting task 3.0 in stage 50.0 (TID 55) (192.168.1.8, executor driver, partition 3, PROCESS_LOCAL, 7817 bytes) \n",
      "24/06/29 12:06:32 INFO Executor: Running task 0.0 in stage 50.0 (TID 52)\n",
      "24/06/29 12:06:32 INFO Executor: Running task 1.0 in stage 50.0 (TID 53)\n",
      "24/06/29 12:06:32 INFO Executor: Running task 2.0 in stage 50.0 (TID 54)\n",
      "24/06/29 12:06:32 INFO Executor: Running task 3.0 in stage 50.0 (TID 55)\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:06:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:06:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:06:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:06:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:06:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:06:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:06:32 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:06:32 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:06:32 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/06/29 12:06:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"BatchId\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TaskName\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceConnection\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceDatabase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TargetTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"StartTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"EndTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceRowsRead\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetInserted\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetUpdated\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnMissing\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnNull\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Error\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Phase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional int32 BatchId;\n",
      "  optional binary TaskName (STRING);\n",
      "  optional binary SourceConnection (STRING);\n",
      "  optional binary SourceDatabase (STRING);\n",
      "  optional binary SourceTable (STRING);\n",
      "  optional binary TargetTable (STRING);\n",
      "  optional binary StartTime (STRING);\n",
      "  optional binary EndTime (STRING);\n",
      "  optional int32 SourceRowsRead;\n",
      "  optional int32 NumTargetInserted;\n",
      "  optional int32 NumTargetUpdated;\n",
      "  optional binary Status (STRING);\n",
      "  optional binary ColumnMissing (STRING);\n",
      "  optional binary ColumnNull (STRING);\n",
      "  optional binary Error (STRING);\n",
      "  optional binary Phase (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:06:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/06/29 12:06:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/06/29 12:06:32 INFO PythonRunner: Times: total = 60, boot = -44424, init = 44484, finish = 0\n",
      "24/06/29 12:06:32 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:06:32 INFO CodecConfig: Compression: SNAPPY\n",
      "24/06/29 12:06:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202406291206321294525237548053395_0050_m_000002_54\n",
      "24/06/29 12:06:32 INFO Executor: Finished task 2.0 in stage 50.0 (TID 54). 2789 bytes result sent to driver\n",
      "24/06/29 12:06:32 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "24/06/29 12:06:32 INFO TaskSetManager: Finished task 2.0 in stage 50.0 (TID 54) in 85 ms on 192.168.1.8 (executor driver) (1/4)\n",
      "24/06/29 12:06:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"BatchId\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TaskName\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceConnection\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceDatabase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"TargetTable\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"StartTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"EndTime\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"SourceRowsRead\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetInserted\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"NumTargetUpdated\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnMissing\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ColumnNull\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Error\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Phase\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional int32 BatchId;\n",
      "  optional binary TaskName (STRING);\n",
      "  optional binary SourceConnection (STRING);\n",
      "  optional binary SourceDatabase (STRING);\n",
      "  optional binary SourceTable (STRING);\n",
      "  optional binary TargetTable (STRING);\n",
      "  optional binary StartTime (STRING);\n",
      "  optional binary EndTime (STRING);\n",
      "  optional int32 SourceRowsRead;\n",
      "  optional int32 NumTargetInserted;\n",
      "  optional int32 NumTargetUpdated;\n",
      "  optional binary Status (STRING);\n",
      "  optional binary ColumnMissing (STRING);\n",
      "  optional binary ColumnNull (STRING);\n",
      "  optional binary Error (STRING);\n",
      "  optional binary Phase (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/06/29 12:06:32 INFO PythonRunner: Times: total = 59, boot = -44426, init = 44484, finish = 1\n",
      "24/06/29 12:06:32 INFO PythonRunner: Times: total = 117, boot = -44426, init = 44543, finish = 0\n",
      "24/06/29 12:06:32 INFO PythonRunner: Times: total = 118, boot = -44426, init = 44544, finish = 0\n",
      "24/06/29 12:06:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202406291206321294525237548053395_0050_m_000001_53\n",
      "24/06/29 12:06:32 INFO Executor: Finished task 1.0 in stage 50.0 (TID 53). 2789 bytes result sent to driver\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:06:32 INFO TaskSetManager: Finished task 1.0 in stage 50.0 (TID 53) in 143 ms on 192.168.1.8 (executor driver) (2/4)\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: Saved output of task 'attempt_202406291206321294525237548053395_0050_m_000003_55' to hdfs://localhost:9000/lakehouse/LH_Global_Electronics_Retailer/Files/log/2024-06-29/batch_8/_temporary/0/task_202406291206321294525237548053395_0050_m_000003\n",
      "24/06/29 12:06:32 INFO SparkHadoopMapRedUtil: attempt_202406291206321294525237548053395_0050_m_000003_55: Committed. Elapsed time: 52 ms.\n",
      "24/06/29 12:06:32 INFO Executor: Finished task 3.0 in stage 50.0 (TID 55). 2875 bytes result sent to driver\n",
      "24/06/29 12:06:32 INFO TaskSetManager: Finished task 3.0 in stage 50.0 (TID 55) in 308 ms on 192.168.1.8 (executor driver) (3/4)\n",
      "24/06/29 12:06:32 INFO FileOutputCommitter: Saved output of task 'attempt_202406291206321294525237548053395_0050_m_000000_52' to hdfs://localhost:9000/lakehouse/LH_Global_Electronics_Retailer/Files/log/2024-06-29/batch_8/_temporary/0/task_202406291206321294525237548053395_0050_m_000000\n",
      "24/06/29 12:06:32 INFO SparkHadoopMapRedUtil: attempt_202406291206321294525237548053395_0050_m_000000_52: Committed. Elapsed time: 64 ms.\n",
      "24/06/29 12:06:32 INFO Executor: Finished task 0.0 in stage 50.0 (TID 52). 2832 bytes result sent to driver\n",
      "24/06/29 12:06:32 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 52) in 331 ms on 192.168.1.8 (executor driver) (4/4)\n",
      "24/06/29 12:06:32 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool \n",
      "24/06/29 12:06:32 INFO DAGScheduler: ResultStage 50 (save at NativeMethodAccessorImpl.java:0) finished in 0.388 s\n",
      "24/06/29 12:06:32 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/29 12:06:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 50: Stage finished\n",
      "24/06/29 12:06:32 INFO DAGScheduler: Job 40 finished: save at NativeMethodAccessorImpl.java:0, took 0.392712 s\n",
      "24/06/29 12:06:32 INFO FileFormatWriter: Start to commit write Job 5a5764a1-decd-4b3f-92b9-5f4674abeb64.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:06:33 INFO FileFormatWriter: Write Job 5a5764a1-decd-4b3f-92b9-5f4674abeb64 committed. Elapsed time: 183 ms.\n",
      "24/06/29 12:06:33 INFO FileFormatWriter: Finished processing stats for write job 5a5764a1-decd-4b3f-92b9-5f4674abeb64.\n"
     ]
    }
   ],
   "source": [
    "# Read metadata action\n",
    "# metadata_action = metadata.read_metadata_action(\"admin\", \"admin\", \"metadata\", \"config_table\", \\\n",
    "#                                                 \"CusDB -> Bronze\")\n",
    "\n",
    "from airflow.models import Variable\n",
    "metadata_action = Variable.get(key = \"metadata_action\", deserialize_json=True, default_var=None)\n",
    "\n",
    "# Define for log job\n",
    "batch_run = hdfsUtils.check_batch_run(project_name, executionDate)\n",
    "start_time = \"\"\n",
    "end_time = \"\"\n",
    "error = \"\"\n",
    "status = \"\"\n",
    "source_row_read = 0\n",
    "numInserted = 0\n",
    "numUpdated = 0\n",
    "\n",
    "\n",
    "# Define parameter for connect to MySQL\n",
    "database = \"Global_Electronics_Retailer\"\n",
    "dbname = f\"jdbc:mysql://localhost:3306/{database}\"\n",
    "driver = \"com.mysql.jdbc.Driver\"\n",
    "username = \"root\"\n",
    "password = \"password\"\n",
    "\n",
    "\n",
    "# tblNames\n",
    "tblNames = [\"customers\", \"sales\", \"products\", \"stores\", \"exchange_rates\"]\n",
    "\n",
    "\n",
    "# # Read all table\n",
    "for metadata in metadata_action:\n",
    "\n",
    "    task_id = metadata[\"task_id\"]\n",
    "    task_name = metadata[\"task_name\"]\n",
    "    source_connection = metadata[\"source_connection\"]\n",
    "    source_database = metadata[\"source_database\"]\n",
    "    source_table = metadata[\"source_table\"].lower()\n",
    "    phase = metadata[\"phase\"]\n",
    "\n",
    "    # Start time for check\n",
    "    start_time = spark.sql(''' SELECT CURRENT_TIMESTAMP() as current_time ''') \\\n",
    "                        .collect()[0][\"current_time\"].strftime('%Y-%m-%d %H:%M:%S')\n",
    "    try:\n",
    "        # Read data\n",
    "        df = extraction.read_table_mysql(spark, driver, dbname, source_table, username, password)\n",
    "        \n",
    "\n",
    "        # Validate data\n",
    "        source_row_read = df.count()\n",
    "        numInserted = df.count()\n",
    "\n",
    "        # Create new column for partition\n",
    "        df = extraction.create_year_month_day(df, executionDate, f)\n",
    "        \n",
    "        # Display df\n",
    "        # df.show()\n",
    "\n",
    "        # Write data to HDFS\n",
    "        code = hdfsUtils.check_exist_data(executionDate, project_name, source_table)\n",
    "        # Exist file\n",
    "        if code == 0: # Yes => Append for version data\n",
    "            df.write.mode(\"append\").format(\"parquet\") \\\n",
    "                    .save(f\"{file_path}/{source_table}/year={year}/month={month}/day={day}/{source_table}_{year}_{month}_{day}-version_{batch_run}.parquet\")\n",
    "        else: # No => First run\n",
    "            df.write.mode(\"overwrite\").format(\"parquet\") \\\n",
    "                    .save(f\"{file_path}/{source_table}/year={year}/month={month}/day={day}/{source_table}_{year}_{month}_{day}-version_{batch_run}.parquet\")\n",
    "    \n",
    "    except:\n",
    "        error = traceback.format_exc()\n",
    "        status = \"Failed\"\n",
    "\n",
    "        print(\"Task \", task_id, \" \", status)\n",
    "\n",
    "    else:\n",
    "        error = \"\"\n",
    "        status = \"Success\"\n",
    "        print(\"Task \", task_id, \" \", status)\n",
    "    \n",
    "    # End time for check\n",
    "    end_time = spark.sql(''' SELECT CURRENT_TIMESTAMP() as current_time ''') \\\n",
    "                        .collect()[0][\"current_time\"].strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Check status\n",
    "    # print(\"Tablename: \", tblName, \"Error: \", error, \"Status: \", status, \n",
    "    #       \"Source rows: \", source_row_read, \"Num of rows Inserted: \", numInserted)\n",
    "\n",
    "\n",
    "    df_log = logUtils.log_data(batch_run, task_name, source_connection, source_database, source_table, \"parquet\",\n",
    "                 start_time, end_time, source_row_read, numInserted, numUpdated, \"\", \n",
    "                 \"\", error, status, phase, t, spark)\n",
    "\n",
    "    df_log.write.mode(\"append\").format(\"parquet\").save(f\"{log_path}/{executionDate}/batch_{batch_run}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 339.177302,
   "end_time": "2024-06-29T05:06:36.733699",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/thanhphat/PersonalProject/Global_Electronics_Retailer/source/NB_source_to_bronze.ipynb",
   "output_path": "/home/thanhphat/PersonalProject/Global_Electronics_Retailer/source/NB_output_phase_1.ipynb",
   "parameters": {},
   "start_time": "2024-06-29T05:00:57.556397",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}