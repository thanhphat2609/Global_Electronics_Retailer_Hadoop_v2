{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config connection to Apache Spark local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(spark_home = \"/home/thanhphat/BigData/spark-3.5.0-bin-hadoop3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import traceback\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"Global_Electronics_Retailer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/26 21:55:55 WARN Utils: Your hostname, thanhphat-inspiron-5406-2n1 resolves to a loopback address: 127.0.1.1; using 192.168.1.7 instead (on interface wlp0s20f3)\n",
      "24/06/26 21:55:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/06/26 21:55:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1719413760041'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.app.startTime', '1719413757653'),\n",
       " ('spark.master', 'local[10]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.jars', '../driver/mysql-connector-j-8.1.0.jar'),\n",
       " ('spark.sql.shuffle.partitions', '100'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///home/thanhphat/PersonalProject/Global_Electronics_Retailer/driver/mysql-connector-j-8.1.0.jar'),\n",
       " ('spark.app.name', 'Source_to_Bronze'),\n",
       " ('spark.sql.parquet.vorder.enabled', 'true'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.memory', '2g'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.host', '192.168.1.7'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://192.168.1.7:45473/jars/mysql-connector-j-8.1.0.jar'),\n",
       " ('spark.driver.port', '45473'),\n",
       " ('spark.app.submitTime', '1719413757285'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder.master(\"local[10]\") \\\n",
    "                            .appName(\"Source_to_Bronze\") \\\n",
    "                            .config(\"spark.sql.parquet.vorder.enabled\", \"true\") \\\n",
    "                            .config(\"spark.sql.shuffle.partitions\", 100) \\\n",
    "                            .config(\"spark.driver.memory\", \"2g\") \\\n",
    "                            .config(\"spark.executor.memory\", \"2g\") \\\n",
    "                            .config(\"spark.jars\", \"../driver/mysql-connector-j-8.1.0.jar\") \\\n",
    "                            .getOrCreate()\n",
    "\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.Extraction import *\n",
    "from modules.HDFSUtils import *\n",
    "from modules.LogUtils import *\n",
    "\n",
    "\n",
    "# Instance for modules\n",
    "extraction = Extraction()\n",
    "hdfsUtils = HDFSUtils()\n",
    "logUtils = LogUtils() \n",
    "\n",
    "# Define base_path\n",
    "file_path = f\"hdfs://localhost:9000/lakehouse/LH_{project_name}/Files/Bronze\"\n",
    "log_path = f\"hdfs://localhost:9000/lakehouse/LH_{project_name}/Files/log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "executionDate = str(spark.sql(\"SELECT CURRENT_DATE()\").collect()[0][0])\n",
    "\n",
    "# Partition Execution Date\n",
    "parse_execution = executionDate.split(\"-\")\n",
    "year = parse_execution[0]\n",
    "month = parse_execution[1]\n",
    "day = parse_execution[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully!!!\n"
     ]
    }
   ],
   "source": [
    "from bson.json_util import dumps\n",
    "import json\n",
    "from pymongo import MongoClient \n",
    "\n",
    "\n",
    "# Read Metadata\n",
    "connection_mongo = \"mongodb+srv://admin:admin@mongo-cluster.r5jfxdp.mongodb.net/metadata?retryWrites=true&w=majority&appName=mongo-cluster\"\n",
    "\n",
    "# Connection to MongoDB  \n",
    "try: \n",
    "    mongo_uri = connection_mongo\n",
    "    client = MongoClient(mongo_uri)\n",
    "    print(\"Connected successfully!!!\") \n",
    "except:   \n",
    "    print(\"Could not connect to MongoDB\") \n",
    "\n",
    "# Connect Database \n",
    "db = client.metadata \n",
    "  \n",
    "# Connect Metadata.config_table \n",
    "collection = db.config_table \n",
    "\n",
    "# Query data with phase: CusDB -> Bronze\n",
    "cursor = collection.find({\"phase\": \"CusDB -> Bronze\"})\n",
    "\n",
    "# Convert to json_data\n",
    "json_data = dumps(cursor, indent = 2)\n",
    "\n",
    "metadata_action = json.loads(json_data)\n",
    "\n",
    "# print(metadata_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[CustomerKey: int, Gender: string, Name: string, City: string, State Code: string, State: string, Zip Code: string, Country: string, Continent: string, Birthday: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Define parameter for connect to MySQL\n",
    "# database = \"Global_Electronics_Retailer\"\n",
    "# dbname = f\"jdbc:mysql://localhost:3306/{database}\"\n",
    "# driver = \"com.mysql.jdbc.Driver\"\n",
    "# username = \"root\"\n",
    "# password = \"password\"\n",
    "\n",
    "# df = extraction.read_table_mysql(spark, driver, dbname, \"customers\", username, password)\n",
    "\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Table Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ls: `hdfs://localhost:9000/lakehouse/LH_Global_Electronics_Retailer/Files/log/2024-06-26/': No such file or directory\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task  1   Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task  2   Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task  3   Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task  4   Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task  5   Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define for log job\n",
    "batch_run = hdfsUtils.check_batch_run(project_name, executionDate)\n",
    "start_time = \"\"\n",
    "end_time = \"\"\n",
    "error = \"\"\n",
    "status = \"\"\n",
    "source_row_read = 0\n",
    "numInserted = 0\n",
    "numUpdated = 0\n",
    "\n",
    "\n",
    "# Define parameter for connect to MySQL\n",
    "database = \"Global_Electronics_Retailer\"\n",
    "dbname = f\"jdbc:mysql://localhost:3306/{database}\"\n",
    "driver = \"com.mysql.jdbc.Driver\"\n",
    "username = \"root\"\n",
    "password = \"password\"\n",
    "\n",
    "\n",
    "# tblNames\n",
    "tblNames = [\"customers\", \"sales\", \"products\", \"stores\", \"exchange_rates\"]\n",
    "\n",
    "\n",
    "# # Read all table\n",
    "for metadata in metadata_action:\n",
    "\n",
    "    task_id = metadata[\"task_id\"]\n",
    "    task_name = metadata[\"task_name\"]\n",
    "    source_connection = metadata[\"source_connection\"]\n",
    "    source_database = metadata[\"source_database\"]\n",
    "    source_table = metadata[\"source_table\"].lower()\n",
    "    phase = metadata[\"phase\"]\n",
    "\n",
    "    # Start time for check\n",
    "    start_time = spark.sql(''' SELECT CURRENT_TIMESTAMP() as current_time ''') \\\n",
    "                        .collect()[0][\"current_time\"].strftime('%Y-%m-%d %H:%M:%S')\n",
    "    try:\n",
    "        # Read data\n",
    "        df = extraction.read_table_mysql(spark, driver, dbname, source_table, username, password)\n",
    "        \n",
    "\n",
    "        # Validate data\n",
    "        source_row_read = df.count()\n",
    "        numInserted = df.count()\n",
    "\n",
    "        # Create new column for partition\n",
    "        df = extraction.create_year_month_day(df, executionDate, f)\n",
    "        \n",
    "        # Display df\n",
    "        # df.show()\n",
    "\n",
    "        # Write data to HDFS\n",
    "        code = hdfsUtils.check_exist_data(executionDate, project_name, source_table)\n",
    "        # Exist file\n",
    "        if code == 0: # Yes => Append for version data\n",
    "            df.write.mode(\"append\").format(\"parquet\") \\\n",
    "                    .save(f\"{file_path}/{source_table}/year={year}/month={month}/day={day}/{source_table}_{year}_{month}_{day}-version_{batch_run}.parquet\")\n",
    "        else: # No => First run\n",
    "            df.write.mode(\"overwrite\").format(\"parquet\") \\\n",
    "                    .save(f\"{file_path}/{source_table}/year={year}/month={month}/day={day}/{source_table}_{year}_{month}_{day}-version_{batch_run}.parquet\")\n",
    "    \n",
    "    except:\n",
    "        error = traceback.format_exc()\n",
    "        status = \"Failed\"\n",
    "\n",
    "        print(\"Task \", task_id, \" \", status)\n",
    "\n",
    "    else:\n",
    "        error = \"\"\n",
    "        status = \"Success\"\n",
    "        print(\"Task \", task_id, \" \", status)\n",
    "    \n",
    "    # End time for check\n",
    "    end_time = spark.sql(''' SELECT CURRENT_TIMESTAMP() as current_time ''') \\\n",
    "                        .collect()[0][\"current_time\"].strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Check status\n",
    "    # print(\"Tablename: \", tblName, \"Error: \", error, \"Status: \", status, \n",
    "    #       \"Source rows: \", source_row_read, \"Num of rows Inserted: \", numInserted)\n",
    "\n",
    "\n",
    "    df_log = logUtils.log_data(batch_run, task_name, source_database, source_table,\n",
    "                 start_time, end_time, source_row_read, numInserted, numUpdated, \"\", \n",
    "                 \"\", error, status, phase, t, spark)\n",
    "\n",
    "    df_log.write.mode(\"append\").format(\"parquet\").save(f\"{log_path}/{executionDate}/batch_{batch_run}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
